{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello!","text":"<p>I'm Rob, and this is my website! It's mostly work-related, but I also record other things here, like the books I read.</p> <p>Created on 2022-02-16</p> <p>Updated on 2024-09-02</p>"},{"location":"work/","title":"Work","text":"<p>I work as a software engineer at Gradle where I am helping to build Develocity.</p> <p>Created on 2022-02-16</p> <p>Updated on 2022-07-07</p>"},{"location":"reading/articles/","title":"Articles","text":""},{"location":"reading/articles/#lockharts-lament","title":"Lockhart's Lament","text":""},{"location":"reading/articles/#barrels-and-ammunition","title":"Barrels and Ammunition","text":""},{"location":"reading/articles/#things-you-should-never-do-part-1","title":"Things you should never do, Part 1","text":"<p>Rewriting from scratch is almost never a good idea.</p>"},{"location":"reading/articles/#founder-mode-and-the-art-of-mythmaking","title":"\"Founder mode\" and the art of Mythmaking","text":""},{"location":"reading/articles/#distributed-systems-shibboleths","title":"Distributed Systems Shibboleths","text":""},{"location":"reading/articles/#devex-what-actually-drives-productivity","title":"DevEx: What Actually Drives Productivity","text":""},{"location":"reading/articles/#a-human-centered-approach-to-developer-productivity","title":"A human-centered approach to developer productivity","text":""},{"location":"reading/articles/#the-space-of-developer-productivity","title":"The SPACE of developer productivity","text":""},{"location":"reading/articles/#write-better-error-messages","title":"Write better error messages","text":"<p>Describes the failures of bad error messages, and also gives an opinionated view on what good looks like, which I think is a pretty good one. If all error message writers learned the lessons described, the world's software would be significantly better.</p>"},{"location":"reading/articles/#questioning-vs-asking","title":"Questioning vs Asking","text":"<p>Asking questions from a place of genuine curiosity has been a game changer for my own learning, and for my work relationships. Sometimes, as you're about to ask a question (e.g. in an email or a message) it is as simple as tuning into your own thoughts and adjusting your intention, without even changing the words you're using. In doing so, we can tap into the part of our selves that is focused on learning, rather than winning.</p>"},{"location":"reading/articles/#the-plan","title":"The Plan","text":""},{"location":"reading/articles/#thinking-about-the-complexity-of-the-kubernetes-ecosystem","title":"Thinking about the complexity of the Kubernetes ecosystem","text":"<p>I like this empathic treatment of Kubernetes from the perspective of new users, experienced users, and of the its creators. I think this empathy bears applicability to other domains as well, such as build tools.</p>"},{"location":"reading/articles/#using-gradle-to-download-and-run-anything","title":"Using Gradle to download and run anything","text":"<p>I like this little trick. I wish the official Gradle documentation had more things like this. I should add it as a sample.</p>"},{"location":"reading/articles/#project-loom-and-thread-fairness","title":"Project Loom and Thread Fairness","text":""},{"location":"reading/articles/#thinking-about-the-complexity-of-the-kubernetes-ecosystem_1","title":"Thinking about the complexity of the kubernetes ecosystem","text":"<p>I like this article for its balanced take on the kubernetes experience, and the project's role in the tooling ecosystem for deploying applications on cloud services.</p>"},{"location":"reading/articles/#cupid-for-joyful-coding","title":"CUPID - for joyful coding","text":"<p>Joyful is a great word to describe the feeling you can give to yourself and your colleagues by writing code with care. Read the corresponding paragraph of the article.</p> <p>The CUPID properties themselves are great. Something I like is that even for one class in an object-oriented design, you can look at it through the lens of these values. I don't find that the same is always true for the SOLID acronym.</p>"},{"location":"reading/articles/#lmax-disruptor","title":"LMAX disruptor","text":""},{"location":"reading/articles/#distributed-systems-shibboleths_1","title":"Distributed Systems Shibboleths","text":"<p>I really liked the positive shibboleths here. In particular:</p> <ul> <li>We made the operation idempotent</li> <li>The system makes incremental progress</li> <li>Every component is crash-only</li> </ul> <p>These are all great qualities for any piece of software to have: Idempotent operations, incremental progress, and uncompromising, early, failures.</p>"},{"location":"reading/articles/#chesire-yeomanry","title":"Chesire Yeomanry","text":"<p>Tech perhaps could learn from the Yeomanry.</p> <ol> <li>Mission command</li> </ol> <p>People ... have a mistaken idea that the Army is rigidly hierarchical. Yes, it\u2019s always extremely clear who\u2019s in command and we have etiquette, symbols and ceremonies to reinforce this, but the Army and especially the Yeomanry is actually excellent at integrating everyone\u2019s input and empowering people at all levels. It\u2019s sacrosanct that I tell the people in my Squadron what I want them to achieve, and not how to achieve it. They take a goal from me and then use their own initiative to make it happen. This is \u2018mission command\u2019 and violating it and micromanaging my Yeomen is a real taboo. If you try to tell a Yeoman how to cross a piece of ground in their vehicle rather than telling them where to get to they\u2019ll certainly let you know what their job is and what your job is. I feel like tech could really learn something from this.</p> <ol> <li>The importance of explaining why</li> </ol> <p>... People think the Army is all about just being told what to do and doing it without question. Really, the Army is fastidious about telling people why they\u2019ve been told to achieve something. In our way of delivering orders we emphasise explaining the context two levels up. I may tell my soldiers to raid a compound, but I would also tell them that the reason for this is to create a distraction so that the Colonel can divert the enemy away from a bridge, and that the reason the Brigadier wants the Colonel to divert the enemy is so that the bridge is easier to cross. Not only do the soldiers then know why it\u2019s important to raid the compound (so that others can cross the bridge), but they know that if for some reason they can\u2019t raid the compound, creating any other diversion or distraction will do in a pinch, and if they can\u2019t do that they can still try to do something to make it easier to cross the bridge. It lets everyone adapt to change as it happens without additional instruction if they aren\u2019t able to get in touch with me. Again I think tech could possibly learn from that.</p>"},{"location":"reading/articles/#the-tyrany-of-what-if-it-changes","title":"The Tyrany of 'what if it changes?'","text":"<p>I don't like the Java-bashing, but this article touches on an important skill of good software engineers - accurately estimating the cost of hypothetical, future change.</p> <p>Example 1: An essential implementation of some specification in your stack is notoriously slow at releasing security patches. It might make sense to decouple from this tool quickly. This might be your choice of JDK.</p> <p>Example 2: The S3 bucket path for storing some data is hard coded. You probably don't need to care about the risk of this changing. If it needs to change, you will deal with it, no problem.</p>"},{"location":"reading/articles/#founders-should-think-about-channeloffer-fit-instead-of-productmarket-fit","title":"Founders should think about channel/offer fit instead of product/market fit","text":""},{"location":"reading/articles/#bashfaq045-mutual-exclusion-in-bash","title":"BashFAQ/045 - Mutual exclusion in Bash","text":"<p>In your darkest hour, you may need this.</p>"},{"location":"reading/articles/#why-i-like-java","title":"Why I like Java","text":"<p>I don't like this article, but I like this cherry-picked excerpt, because it describes a programming feeling that I like: The feeling that your software is a just matter of time. Of course, I do take great care to see that I am doing things in the right way.</p> <p>In Haskell or even in Perl you are always worrying about whether you are doing something in the cleanest and the best way. In Java, you can forget about doing it in the cleanest or the best way, because that is impossible. Whatever you do, however hard you try ... the only thing you can do is relax and keep turning the crank until the necessary amount of code has come out of the spout.</p>"},{"location":"reading/articles/#minsky-moments-in-venture-capital","title":"Minsky moments in Venture Capital","text":"<p>Created on 2022-05-28</p> <p>Updated regularly</p>"},{"location":"reading/books/","title":"Books","text":"<p>Books I've read, liked, and might recommend.</p>"},{"location":"reading/books/#software-engineering","title":"Software engineering","text":""},{"location":"reading/books/#coding","title":"Coding","text":"<ul> <li>Growing Object-Oriented Software, Guided by Tests - Steve Freeman and Nat Pryce</li> <li>Working Effectively with Legacy Code - Michael Feathers</li> <li>Design Patterns: Elements of Reusable Object-Oriented Software - Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides</li> </ul>"},{"location":"reading/books/#organisational","title":"Organisational","text":"<ul> <li>eXtreme Programming Explained, 2nd edition - Kent Beck and Cynthia Andres</li> <li>Accelerate - Nicole Forsgren, Jez Humble, Gene Kim</li> <li>Team Topologies - Matthew Skelton and Manuel Pais</li> <li>Cognition in the Wild - Edwin Hutchins</li> </ul>"},{"location":"reading/books/#communication","title":"Communication","text":"<ul> <li>Crucial Conversations - Kerry Patternson, Joseph Grenny, Ron McMillan, Al Switzler</li> <li>Nonviolent Communication, 3rd edition - Marshall Rosenberg</li> <li>Agile Conversations - Douglas Squirrel and Jeffrey Frederick</li> <li>Bugs in Writing: A Guide to Debugging your Prose - Lyn Dupre</li> </ul>"},{"location":"reading/books/#humanities","title":"Humanities","text":""},{"location":"reading/books/#history","title":"History","text":"<ul> <li>Black Wave - Kim Ghattas</li> <li>From third world to first: The Singapore Story, 1965-2000 - Lee Kuan Yew</li> <li>Truth-Telling - Henry Reynolds</li> <li>India: A History - John Keay</li> <li>Map of a Nation: A Biography of the Ordnance Survey - Rachel Hewitt</li> <li>Longitude - Dava Sobel</li> </ul>"},{"location":"reading/books/#economics","title":"Economics","text":"<ul> <li>Introductory Economics (5th edition) - George Frederick Stanlake</li> <li>The bottom billion - Paul Collier</li> </ul>"},{"location":"reading/books/#health","title":"Health","text":"<ul> <li>The Body Keeps the Score - Bessel Van Der Kolk</li> <li>When the body says no - Gabor Mat\u00e9</li> <li>Gut - Giulia Enders</li> </ul>"},{"location":"reading/books/#narrative-non-fiction","title":"Narrative non-fiction","text":"<ul> <li>The Boys in the Boat - Daniel James Brown</li> <li>Endurance - Alfred Lansing</li> </ul>"},{"location":"reading/books/#fiction","title":"Fiction","text":"<ul> <li>The Curious Incident of the Dog in the Night Time - Mark Haddon</li> <li>The Hitchhiker's Guide to the Galaxy - Douglas Adams</li> <li>The daughter of time - Josephine Tey</li> <li>And then there were none - Agatha Christie</li> <li>The Garbage King - Elizabeth Laird</li> </ul> <p>Created on 2022-05-28</p> <p>Updated regularly</p>"},{"location":"reading/log/","title":"Log","text":"<p>The month in each entry refers to when I finished reading a book. Generally I start a new book right after finishing a previous one. I only read one book at a time.</p>"},{"location":"reading/log/#2025","title":"2025","text":"<ul> <li>Wip: Understanding cancer - Robin Hesketh</li> <li>Apr: The palace of illusions - Chitra Banarjee Divakaruni</li> <li>Apr: The fault in our stars - John Green</li> <li>Apr: And then there were none - Agatha Christie</li> <li>Apr: Why we sleep - Matthew Walker</li> <li>Mar: When the body says no: The cost of hidden stress - Gabor Mat\u00e9</li> <li>Jan: Gut - Giulia Enders</li> </ul>"},{"location":"reading/log/#2024","title":"2024","text":"<ul> <li>Dec: Technological revolutions and financial capital - Carlota Perez</li> <li>Oct: The daughter of time - Josephine Tey</li> <li>Oct: The adventure of the Christmas pudding - Agatha Christie</li> <li>Oct: The intelligent investor - Benjamin Graham</li> <li>Jul: The bottom billion - Paul Collier</li> <li>Jun: From third world to first: The Singapore Story, 1965-2000 - Lee Kuan Yew</li> <li>Apr: Black wave - Kim Ghattas</li> <li>Mar: Longitude - Dava Sobel</li> <li>Mar: The curious incident of the dog in the night time - Mark Haddon</li> <li>Mar: Coolies of the empire: Indentured indians of the sugar colonies, 1830-1920 - Ashutosh Kumar</li> <li>Jan: The subtle art of not giving a f*ck - Mark Manson</li> </ul>"},{"location":"reading/log/#2023","title":"2023","text":"<ul> <li>Dec: A life on our planet - David Attenborough</li> <li>Dec: A history of Scotland - Rosalind Mitchison</li> <li>Aug: No bad parts - Richard Schwartz</li> <li>Jul: The body keeps the score - Bessel Van Der Kolk</li> <li>May: No mountain too high - Kumaran Rasappan</li> <li>Apr: Truth-Telling - Henry Reynolds</li> <li>Mar: The three-body problem - Cixin Liu</li> <li>Jan: Map of a nation - Rachel Hewitt</li> </ul>"},{"location":"reading/log/#2022","title":"2022","text":"<ul> <li>Nov: India: A history - John Keay</li> </ul> <p>Created on 2023-01-14</p> <p>Updated regularly</p>"},{"location":"reading/selected-snippets/","title":"Selected snippets","text":"<p>This page contains some quotations from books and other sources, which resonated with me when I first encountered them.</p>"},{"location":"reading/selected-snippets/#selection","title":"Selection","text":"<p>Make the change easy, then make the easy change.</p> <ul> <li>Kent Beck</li> </ul> <p>This describes how I work, and have worked for a number of years now. It is a wonderful way of building.</p> <p>My early experiences in Singapore and Malaya shaped my views about the claim of the press to be the defender of truth and freedom of speech. The freedom of the press was the freedom of its owners to advance their personal and class interests.</p> <ul> <li>Lee Kuan Yew in his memoir, From Third World to First: The Singapore Story 1965 - 2000</li> </ul> <p>Consider the state of journalism, popular media and adult education in the UK, and how different our situation might be today if these things were not treated as vehicles for the ultra rich (e.g. Rupert Murdoch) and foreign actors (e.g. Putin) to advance their personal and class interests at the expense of British people.</p> <p>Our culture emphasizes that as leaders we must be wiser, set direction, and articulate values, all of which predisposes us to tell, rather than ask.</p> <ul> <li>Edgar H. Schein in Humble Inquiry: The Gentle Art of Asking instead of Telling</li> </ul> <p>When I read this it reminded me immediately of one of principles of Sydney Dekker's Safety Differently. Instead of telling people what to do, we can empower them to make safer and therefore more performant systems by asking people what they need.</p> <p>Safety currently: - People are the problem - We should tell them what to do - We measure success by the absence of negatives (e.g. counting safety incidents)</p> <p>Safety differently: - People are the solution - We should ask them what they need - We measure success by counting our positive capabilities (e.g. enumerating them)</p> <ul> <li>Sydney Dekker, In a slide of a 'Safety Differently' lecture</li> </ul> <p>This slide has significantly impacted my understanding of safety in software. In theory, precisely prescribed processes promise protection against safety incidents. Yet in practice, such prescription breaks down in the face of the world's complexity, and we are forced to deal with the lived realities of people working on the ground in order to make progress on safety. I believe software development has so much to gain from this outlook on safety, if only it were adopted more widely.</p> <p>Created on 2022-08-19</p> <p>Updated regularly</p>"},{"location":"reading/videos/","title":"Videos","text":"<p>Here are some talks I like:</p>"},{"location":"reading/videos/#postcards-from-the-peak-of-complexiy-brian-goetz","title":"Postcards from the peak of complexiy - Brian Goetz","text":"<p>I found this relateable, and it made me think about wide-scoped solutions and frameworks for solving a problem across the engineering organisation, that I in my ignorance shipped at the peak of comlexity, and how I and others paid for that in time. I loved the metaphor of a \"virtuous collapse\" after reaching the peak of complexity.</p>"},{"location":"reading/videos/#aspiring-to-learn-the-engineering-of-software-alan-kay-1-hour","title":"Aspiring to learn the engineering of software - Alan Kay (~1 hour)","text":"<p>This made me think about the level of understanding we have as an industry about the human process of creating software. Sometimes millions are spent, thousands of people are enlisted, yet the result utterly fails. Other times hardly anything is spent, just a handful of people are involved and they are able to change the world. Software engineering has not yet reached the point of being science.</p> <p>Another point I loved: Nobody fabricates computer chips before they are known to work via simulation. The physical engineering disciplines all follow a similar process of: CAD (Computer Aided Design) =&gt; SIM (Simulate) =&gt; FAB (Fabricate i.e. build). For example, the manufacter of aeroplanes, buildings, bridges etc. When it comes to software engineering though, we are so caught up in FAB that we hardly design or simulate what we're building. In the talk, Kay notes the irony of software engineers making better design tools for other disciplines than they are willing to make for themselves.</p> <p>Throughout the talk, the importance of ethics and safety in software engineering is emphasised, and the talk ends with a mention of the hippocratic oath of medicine, and the noticable lack of an equivalent in software engineering.</p>"},{"location":"reading/videos/#riding-the-inflection-point-chan-cheow-hoe-25-mins","title":"Riding the inflection point - Chan Cheow Hoe (~25 mins)","text":"<p>Chan Cheow Hoe is the CTO for GovTech Singapore. In this talk, he talks about what it is like to inherit a large engineering organisation that is not accustomed to delivering at the pace that people expect today, and about the mindsets that helped him. Namely, (1) Adopt a reduction mindset: Reduce complexity, duplication, tech debt, toil; (2) Build accelerators: Products -&gt; Platforms -&gt; Ecosystem; (3) Build competencies: More makers, fewer checkers. More experts, fewer operators. More doers, fewer managers; (4) Automate everything: Reliability -&gt; Repeatability -&gt; Predictability. Early in the talk is the need for scaling (superlinear) compared to what many organisations do instinctively, which is merely growing (linear).</p>"},{"location":"reading/videos/#dont-make-these-hiring-mistakes-yc-20-mins","title":"Don't make these hiring mistakes - YC (~20 mins)","text":"<p>Coming from a consulting background, I interpreted this with an alternative title: \"Don't make these staffing mistakes\". The advice from the YC partners here aligns very much with my own opinions, based on my experience.</p>"},{"location":"reading/videos/#the-alternate-reality-kit-randall-smith-12-mins","title":"The Alternate Reality Kit - Randall Smith (~12 mins)","text":"<p>A delightful demo from PARC, which for me captures a simple essence of OOP.</p>"},{"location":"reading/videos/#seminar-on-object-oriented-programming-alan-kay-2-hours","title":"Seminar on Object-Oriented programming - Alan Kay (~2 hours)","text":"<p>I found this video by chance, thanks to Hemal Varambhia, who shared it on his LinkedIn. It's great.</p>"},{"location":"reading/videos/#the-software-crisis-and-data-abstraction-barbara-liskov-20-mins","title":"\"The Software Crisis\" and data abstraction - Barbara Liskov (~20 mins)","text":"<p>\"Those who do not learn history are doomed to repeat it\" etc.</p> <p>Created on 2022-06-10</p> <p>Updated regularly</p>"},{"location":"writing/developer-experience/","title":"Developer Experience","text":""},{"location":"writing/developer-experience/#definition","title":"Definition","text":"<p>The term 'developer experience' is a special case of the term 'user experience', to centre software developers specifically as the subject.</p> <p>The original term ('user experience' / UX) is a tautology, and 'developer experience' or DX for short, is also a tautology.</p> <p>The word 'experience' here refers to the lived experiences of the people who are the subject. A special definition of 'developer experience' is unnecessary, because it is exactly what it says it is. Developer experience is the lived experience of software developers. Obviously, it is used in the context of their work. Whatever developers experience while developing software, is part of DX. For example, using a search engine is a common developer experience. This is regardless of whether there is any connection between other development tools and the search engine. Remember, DX means \"the lived experiences of developers\".</p> <p>Sometimes I worry that the industry is going to take a turn towards a place where companies talk about how much they care about developer experience, but what they do is quite plainly not in pursuit of improving the lived experiences of developers at work. Instead, we will return to prioritising processes and tools over individuals and interactions.</p> <p>As a tautology, you would hope the meaning is so obvious that this doesn't happen. History does not offer a reassuring view here though. Other, seemingly inconfusable tautologies, have previously befallen the fate of having their meaning turned upside down. For example, the term continuous integration is now synonymous with discontinuous, branch-based integration.</p>"},{"location":"writing/developer-experience/#who-cares-about-dx","title":"Who cares about DX?","text":"<p>Every company that needs to create software products of any kind, for anyone, needs to think about the lived experience of the developers who build those products, whether they are employees, contractors, customers, or their customers' customers. A recent article I read seemed to suggest that DX is something that only developer tooling companies need to think about. I don't agree with this.</p>"},{"location":"writing/developer-experience/#why-care-about-dx","title":"Why care about DX?","text":""},{"location":"writing/developer-experience/#bullet-points","title":"Bullet points","text":"<p>The importance of DX is not universally intuitive. If it were, I would not be writing this in a huff. Here are some bullet points, which reflect what I think about, when I think about the importance of DX prioritisation.</p> <p>Why would you want to improve the lived experience of the developers who build the software?</p> <ul> <li>Unhappy, unmotivated workers produce worse products, slower. Companies full of such workers achieve worse outcomes compared to competitors with happier, more motivated workers.</li> <li>Good developers leave companies with poor DX, specifically to move towards companies with better DX. The trend is that companies which don't care about DX either lose or lack the talent required to create and maintain competitive software products.</li> <li>Removing obstacles and automating rote tasks (i.e. reducing toil) has great cost-saving and innovation-enabling qualities:<ul> <li>Reducing toil saves time during the day for a developer that they would otherwise spend doing something that a machine can do better. This daily time saving scales across the number of developers affected by the toil. Even if you plug in only small numbers for the time savings, the scaling factor can multiply this into significant savings per unit time.</li> <li>Reducing toil removes cognitive encumbrances on your developers that enable them to focus on important problems more deeply, further boosting productivity.</li> <li>Reducing toil exposes potential opportunities for product or technical innovation, by freeing up time and cognitive ability that is otherwise locked up in unproductive busywork. There is a great opportunity cost associated with leaving toil to build up, without prioritising its continuous removal. I haven't seen this aspect of toil reduction included in existing writings on the subject of DX.</li> </ul> </li> </ul>"},{"location":"writing/developer-experience/#intuition","title":"Intuition","text":"<p>For '99% Developers', their lived experience in their workday is routinely dismissed out of hand, in the unending prioritisation battles that are being fought in their workplaces, such as banks, insurance companies, and the public sector. More on this later.</p> <p>There is research, such as that in Accelerate, which shows statistically significant correlations between prioritisation of fairly minor developer experience improvements, and improved organisational performance across commercially important metrics (e.g. profitability). The lead researcher of this work, Nicole Forsgren, is also an author of the 'SPACE' paper about metrics for developer productivity.</p> <p>Those engineering organisations that are waiting to be spoon-fed the crunched data from researchers, are being left behind by those who intuitively understand that DX matters if you want to compete efficiently in the market of software products.</p> <p>Long before Accelerate, or any widely recognized research in the area had been done, many developers had been talking about the importance of things like version control, continuous integration, and continuous delivery. They would still have continued advocating for them in the absence of formal research such as that done by the DevOps Research and Assessment (DORA) team. For many people, the importance of centering the lived experiences of developers in order to improve their productivity is intuitive. For me it is a basic value that people should be empowered to be able to make their work more productive and enjoyable.</p>"},{"location":"writing/developer-experience/#dx-forces","title":"DX forces","text":"<p>I think the tooling aspect of developer experience is over-represented in the available writings about DX online. This subsection is about a few other aspects.</p> <p>An important aspect of DX that I don't see much written about is the habitability of the codebase. Those few who have been so blessed to work with wonderfully well-tested and thoughtfully designed systems know how much difference to your experience and your productivity it makes, to work on a piece of code with great automated tests, that are cleanly refactored, such that useful abstractions are readily available to set up and write the next automated test. This is helpful when trying to reproduce and fix bugs, when extending an existing feature, and when adding new features.</p> <p>The presence of abstractions in the codebase, independent of tools, is another contributor to DX that I think most companies don't take explicit care of. I don't foresee this entering the cultural mainstream of the industry for while though.</p> <p>Amazing, incredibly productive developer experiences within your codebase don't appear out of nothing. They need to be tended to, every day, especially in fast-growing, or already large teams, where the tendency for divergence is greater.</p> <p>Team structuring and sizing is also important. You can't adjust them too frequently because you'll destabilise your delivery system, but you can occasionally, or even periodically tune them in order to address (proactively or reactively) DX challenges you're facing. I really liked Team Topologies by Mathew Skelton and Manuel Pais on this topic.</p>"},{"location":"writing/developer-experience/#dx-apathy-culture","title":"DX apathy culture","text":"<p>A great number of huge and highly profitable traditional businesses, know that building software products is essential for their continued success.</p> <p>Efforts have yielded mixed results. I have consulted at two banks, whose retail (i.e. consumer-facing) banking apps had ~2 star ratings at the time when I (temporarily) joined. Making products that are compelling, well-designed and free of bugs, is really hard. It's a lot harder when you have a raise a support ticket to download your IDE, or if you have to run and maintain an instance of Mattermost for your team's use because there are no approved internal communication tools aside from Skype messenger (a tool which does not persist messages across client sessions, and therefore is essentially useless for a team).</p> <p>Such businesses struggle to produce good software products, and it is really no surprise. There are a few characteristics of these engineering organisations which I consider \"tell-tale signs\" of a culture that does not care about the experience of developers. I claim here only correlation, and make no claim as to a cause:</p> <ul> <li>Reliant on external software vendors for development</li> <li>Work is organised around projects, and not products</li> <li>Political game-playing and career-driven maneuvering is commonplace aka 'The Hot Potato program management methodology'</li> <li>Highly regulated industry (although the regulations don't usually specify that the software must be awful)</li> <li>Other people, who are not developers, also complain about being unhappy</li> </ul>"},{"location":"writing/developer-experience/#takeaways","title":"Takeaways","text":"<ol> <li>'Developer experience' is a tautology. It refers to the lived experiences of developers.</li> <li>Every company that builds software products of any kind (including apps, websites, eCommerce things etc.) should be interested in fostering great experiences for their developers.</li> <li>It should be intuitively clear that DX is important for the success of a company that builds and depends on its software products for revenue, but if not, there is also compelling data.</li> <li>DX is not only about tools. Codebase habitability, abstraction and team structure are also important.</li> <li>In my experience, companies with the worst DX often have a few other curious traits in common.</li> </ol> <p>Created on 2022-08-28</p> <p>Updated on 2024-08-30</p>"},{"location":"writing/reader-experience/","title":"Reader Experience","text":"<p>The importance of readability in your code is broadly accepted to be one of the keys to maintainability and developer happiness. I think there is something to this, but the focus on \"readability\" is not quite right. When you think about the next person to arrive at your code, as you should, I recommend that you don't think about your code's inherent readability. Rather, think about a person, arriving at the scene of your code:</p> <ul> <li>What do they know? What might they not be aware of?</li> <li>How experienced are they in the technology you're using?</li> <li>Are they looking for something?</li> <li>Which team are they in?</li> <li>Why are they here?</li> </ul> <p>I think focusing on future readers and authors of your code, makes you more effective and sensitive in your efforts to improve maintainability and productivity through refactoring.</p> <p>Thinking about \"readability\" centers the code, and puts you at risk of losing the reader. Thinking about \"reader experience\" centers the reader.</p> <p>Part of thinking about your reader's experience, is thinking about what readers are trying to achieve when they arrive at your code. Are they just trying to understand, or are they also trying to edit, simplify, delete or move? You can optimize separately for those different experiences, depending on the usage patterns you anticipate for the code you're writing.</p> <p>When thinking about experiences related to changing the code, as opposed to just reading it, I use the term \"author experience\".</p> <p>I think the absolute best way to achieve a high quality reader and author experience, is to write great tests.</p> <p>Created on 2022-06-06</p> <p>Updated on 2022-08-28</p>"},{"location":"writing/tests/","title":"Tests","text":"<p>Writing great tests is an important part of providing a wonderful reader experience and author experience to your colleagues.</p> <p>Here is a rough checklist I made related to writing a great test suite in an object-oriented programming language. I find these principles easy to interpret from the perspective of a non-object-oriented programming language (like Rust or Q) as well.</p> <ol> <li>Tests are as fast as possible, given their scope. Tests that have a noticable total execution time are scrutinized.</li> <li>Tests are deterministic. The test author makes a reasonable effort to preempt and detect flakiness in the tests they write.</li> <li>A test method fails for exactly one reason. That is, there is only one assertion. Tests do not \"ramble\", with many actions and assertions. Tests like this are broken up into multiple tests.</li> <li>A unit test class contains tests both for expected, successful outcomes and any other potential outcomes. The former are more important, because they prove that the unit under test can ever possibly work, if nothing goes wrong at all. Later tests, for exceptional or unexpected behaviours are also important, because they help future readers and authors understand exactly how the unit behaves under a diverse range of circumstances.</li> <li>The test author watched the test fail, either because by doing TDD and writing the test before writing the source code that implements the behaviour they are testing, or, by going back and breaking the program's behaviour to make sure the test will actually catch a regression.</li> <li>The test author has honed the test's failure message(s) so that it is easy for others to interpret a failure. The name of the test method, and the name of the test class, also contribute to providing sufficient context, such that the next person to encounter this failing test understands why it failed, what behaviour it is protecting, and how to fix it.</li> <li>Test code is crafted and refactored like production code. Logical entities in the test are extracted into domain-specific classes within the test sources. Duplication is regarded as an opportunity to create meaningful abstractions. Names are deliberated upon, to provide an intuitive reader experience.</li> <li>Tests can be executed locally by developers on their machine, using their own, possibly modified, copy of the code. Use of secret credentials and custom configurations for tests is discouraged, especially if they are present only on CI, and therefore more difficult for new or inexperienced team members to discover.</li> <li>The name of a test should describe a general behaviour that your unit exhibits e.g. \"checks that file exists\" rather than \"throws exception if argument to --file is non-existing file\". Don't put the body of the test in the name of the test.</li> <li>Source code is written with testability in mind. Testability is an essential feature of high-quality production code. Code should be deliberately written in a way that makes it easy for you and other authors to write fast-running automated tests which verify that your program behaves correctly.</li> </ol> <p>Created on 2022-06-07</p> <p>Updated on 2024-01-28</p>"},{"location":"writing/documentation/divio/","title":"Divio's documentation system","text":"<p>A while ago I encountered the Divio documentation system. I really like it for its distinction between different kinds of users and their needs. As a recovering consultant, I confess to also loving the quadrant diagram.</p> <p>This system is a great starting point, but there are many possible extensions we can apply to it. I first saw this noted by Hillel Wayne in his article: Beyond the Four-document model, in which he describes his vision for \"Conceptual Overview\" and \"Examples\" areas of documentation. At the end he briefly mentions a few more: \"FAQs\", \"Troubleshooting\" and \"Best practices\". A colleague of mine at Gradle also mentioned that she used \"Cheat sheet\" as an extension.</p> <p>Regardless of what particular flavour of this model you use, this more principled way of structuring documentation is certainly a step forward compared to what we have tolerated until now. I find it heartening to see the high level of adoption and passion for better documentation models amongst software maintainers.</p> <p>Created on 2022-08-05</p> <p>Updated on 2022-08-05</p>"},{"location":"writing/documentation/values/","title":"Documentation: Values","text":""},{"location":"writing/documentation/values/#my-values-for-technical-documentation","title":"My values for technical documentation","text":""},{"location":"writing/documentation/values/#respect","title":"Respect","text":"<p>Summary: Precise, detailed documentation is not burdensome to users in need of information. Treat them with respect, and give them complete, accurate information which reflects the best of your knowledge and experience.</p> <p>There is a trope in software, that users are clueless. I have sympathy for this perspective. Too often though, documentation authors, in efforts to work around the perceived incompetence of their users, omit important details, deemed to be \"too low-level\" or \"too complicated\" for their readers. Authors express a desire to \"shield\" users from \"excessive detail\".</p> <p>Authors are concerned that detailed, precise documentation will bewilder users, because it is verbose. Or that too many examples will bore users if not every example is relevant to them. As a result of being driven by this fear, documentation authors produce works which lack depth. These works are unhelpful for advanced users, and don't provide a path for less-advanced users to advance beyond their current level of understanding.</p> <p>Don't omit, hide, sanitize, or censor the full, accurate, true information about your products. Entrust your users with the ability to use and configure the product as much as is possible i.e. as much as you can and do.</p>"},{"location":"writing/documentation/values/#clarity-over-brevity","title":"Clarity over brevity","text":"<p>Summary: When you can't come up with a succinct phrasing to capture an idea, don't be afraid to be verbose. Apply rhetorical techniques where they aid clarity and make information explicit. For example, constrasting with a (dis)similar idea, or reiterating the most important ideas.</p> <p>Related to the problem of not respecting the reader, is the problem of assuming the reader doesn't have time to read a complete sentence. I unfortunately rarely encounter documentation which says things in more than one way, so as to clarify them. Documentation rarely contains rhetorical devices like contrasting or reiterating, because it is assumed to be too verbose, and users will get bored. To me, this is obviously a mistake, because it demonstrates a  prioritisation for brevity over clarity. Users in need, are not asking for less documentation. It sounds really obvious when I put it like that. That's because it should be really obvious.</p> <p>This is not to say that brevity is not important. It is very important. To supply the right amount of information, in the right place, at the right time. Brevity is important. But clarity is more important. If you ever have to choose between your your documentation being brief and being clear, always choose to be clear.</p>"},{"location":"writing/documentation/values/#no-dead-ends","title":"No dead ends","text":"<p>Summary: Include links in your documentation, which connect related aspects of the subject matter in order to provide readers with a more holistic understanding.</p> <p>Sometimes a page will touch on an important subject, that you have been searching for information about. It will contain only a thin veneer of information. There will be no links to other, tangentially related sections, so as to avoid presenting a holistic, coherent picture of your produce or service. Do not do this.</p> <p>Use links, liberally, in order to connect concepts.  This makes it easier for your reader to form a mental model of the subject, and ultimately better understand the content.</p>"},{"location":"writing/documentation/values/#education","title":"Education","text":"<p>Summary: The best documentation not only conveys new information, but it also enhances the reader's understanding of topics that they have already grasped. Don't be afraid to use verbose, precise language to provide readers with different perspectives on systems or tools with which they may already be familiar.</p> <p>It's a delightful experience to learn something new and exciting when you didn't expect to, or as a side effect of learning something else. To foster these experiences, provide tangential educational material inline in your documentation, to allow users to learn more about the ecosystem around the technical topic you are writing about. Inevitably, this will give them a deeper appreciation and understanding of your content.</p> <p>Created on 2022-08-05</p> <p>Updated on 2023-05-25</p>"},{"location":"writing/gradle/custom-repositories/","title":"Custom repositories","text":"<p>Downloading files using Gradle should be really easy, it shouldn't require any third party shit, and it should be compatible with all stable optimization features of the build tool. There is a technique that conforms to these basic constraints. It is pretty convoluted, and I have never seen official documentation for it. We use it in our build in several places, and I have applied it myself. It's satisfying once it works. This page will explain how to download arbitrary files from the internet using Gradle in a way that doesn't require you to apply any plugins, register any tasks and isn't a performance footgun.</p>"},{"location":"writing/gradle/custom-repositories/#terminology","title":"Terminology","text":""},{"location":"writing/gradle/custom-repositories/#ivy","title":"Ivy","text":"<p>Ivy refers to Apache Ivy - a dated but highly influential dependency manager, developed as a subproject of Apache Ant.</p>"},{"location":"writing/gradle/custom-repositories/#configuration","title":"Configuration","text":"<p>The poorly named 'Configuration' in Gradle, is a subtype of the much better-named 'ConfigurableFileCollection'. That is, a Configuration is an object that just represents a bunch of files. That is the most common use of a Configuration, in my experience, and provides the simplest mental model.</p>"},{"location":"writing/gradle/custom-repositories/#resolvable-configuration-vs-consumable-configuration","title":"Resolvable Configuration vs Consumable Configuration","text":"<p>Configurations are either resolvable or consumable.</p> <p>Resolving a configuration refers to taking out its files in order to do something with them. So a resolvable configuration is one that you can directly use as a collection of files. You use resolvable configurations when you want to declare and make use of a dependency.</p> <p>The \"consumable\" in \"consumable configuration\" refers to dependency consumption. A consumable configuration can be exported from one project to another as a project dependency (i.e. <code>implementation(project(\":foo\"))</code>). It can't be resolved, so it isn't useful to the project that produces it. You use consumable configurations when you want to export the output of a task to other projects in your multi-project build.</p>"},{"location":"writing/gradle/custom-repositories/#artifact-transform","title":"Artifact Transform","text":"<p>To be honest I still don't quite understand what a Gradle artifact transform is, but I will present the inevitably incorrect mental model I use (in the spirit of \"all models are wrong, some are useful\"). There is official documentation covering this, which I think explains the concept to some degree.</p> <p>An artifact transform is a Gradle object that knows how to perform an operation on a dependency, and in doing so, alters a piece of its associated metadata. When Gradle resolves a requested dependency, it requests it with certain metadata, and this metadata is used by Gradle to determine whether or not it can satisfy a dependency consumer's request by transforming an available dependency using a registered artifact transform.</p>"},{"location":"writing/gradle/custom-repositories/#custom-repository-declaration","title":"Custom repository declaration","text":"<p>This custom repository declaration tells Gradle that it can find artifacts with the group \"trinodb\" in this specified Ivy repository. It has a base URL and a path pattern, based on the coordinates provided in the dependency block.</p> <p>The URL to download one of the files that this repository declaration downloads is: <code>https://github.com/trinodb/grafana-trino/releases/download/v1.0.6/trino-datasource-1.0.6.zip</code>.</p> <pre><code>repositories {\n    exclusiveContent {\n        forRepository {\n            ivy {\n                url = uri(\"https://github.com/trinodb/grafana-trino\")\n                patternLayout { artifact(\"releases/download/v[revision]/[artifact]-[revision].[ext]\") }\n                metadataSources {\n                    artifact()\n                }\n            }\n        }\n        filter {\n            includeGroup(\"trinodb\")\n        }\n    }\n}\n</code></pre> <p>To download and use the file we want to download, we need to declare a dependency on it. To do that, we need a resolvable configuration to attach our dependency to.</p> <p>This code creates a resolvable configuration, and declares that the files inside it need to be directories. Gradle will not populate this configuration with non-directory files.</p> <pre><code>val trinoDatasourcePlugin by configurations.creating {\n    isCanBeResolved = true\n    isCanBeConsumed = false\n    attributes.attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE)\n}\n</code></pre> <p>This code first registers an in-built artifact transform that converts a zip archive into a directory. The <code>UnzipTransform</code> artifact transform is in-built in Gradle. I'm not sure why you therefore need to declare it explicitly, but evidently you do.</p> <p>After regisering the transform, we declare our dependency. We say that our resolvable configuration has a dependency on an artifact with coordinates \"trinodb:trino-datasource:1.0.6\" and we are expecting a zip file. We also say that we do not want this dependency to be resolved with any of its transitive dependencies, because that doesn't make sense - we are downloading a single file.</p> <pre><code>dependencies {\n    registerTransform(UnzipTransform::class) {\n        from.attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.ZIP_TYPE)\n        to.attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE)\n    }\n\n    trinoDatasourcePlugin(\"trinodb:trino-datasource:1.0.6\") {\n        artifact {\n            type = \"zip\"\n            isTransitive = false\n        }\n    }\n}\n</code></pre> <p>To start actually using the files inside that unzipped directory that we've downloaded, we can register a task that uses the files. For example:</p> <pre><code>tasks.register(\"printDownloadedDirectoryPath\") {\n    val inputDir = configurations.named(\"trinoDatasourcePlugin\").map { it.singleFile }\n    doFirst {\n        logger.lifecycle(\"Found file: \" + inputDir.get().path)\n    }\n}\n</code></pre> <p>When I run this, I see that Gradle downloads the file for me, and the task prints out the text I expected:</p> <pre><code>Starting a Gradle Daemon...\n...\n&gt; :printFiles &gt; Resolve files of configuration ':trinoDatasourcePlugin' &gt; trino-datasource-1.0.6.zip &gt; 9.3 MiB/44.6 MiB downloaded\n...\n&gt; Task :printDownloadedDirectoryPath\nFound file: /Users/rob/.gradle/caches/transforms-4/6494f3751f7959da86816b0d7720b563/transformed/trino-datasource-1.0.6\n</code></pre> <p>If I look inside it, that directory has what I wanted:</p> <pre><code>rob@Robs-MacBook-Pro-2 custom-gradle-repositories-demo % tree /Users/rob/.gradle/caches/transforms-4/6494f3751f7959da86816b0d7720b563/transformed/trino-datasource-1.0.6\n/Users/rob/.gradle/caches/transforms-4/6494f3751f7959da86816b0d7720b563/transformed/trino-datasource-1.0.6\n\u2514\u2500\u2500 trino-datasource\n    \u251c\u2500\u2500 CHANGELOG.md\n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 MANIFEST.txt\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 go_plugin_build_manifest\n    \u251c\u2500\u2500 gpx_Trino_darwin_amd64\n    \u251c\u2500\u2500 gpx_Trino_darwin_arm64\n    \u251c\u2500\u2500 gpx_Trino_linux_amd64\n    \u251c\u2500\u2500 gpx_Trino_linux_arm\n    \u251c\u2500\u2500 gpx_Trino_linux_arm64\n    \u251c\u2500\u2500 gpx_Trino_windows_amd64.exe\n    \u251c\u2500\u2500 img\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 explore.png\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 logo.svg\n    \u251c\u2500\u2500 module.js\n    \u251c\u2500\u2500 module.js.map\n    \u2514\u2500\u2500 plugin.json\n\n3 directories, 16 files\n</code></pre>"},{"location":"writing/gradle/custom-repositories/#notes","title":"Notes","text":""},{"location":"writing/gradle/custom-repositories/#the-artifact-transform-aspect-of-this-was-entirely-optional","title":"The artifact transform aspect of this was entirely optional","text":"<p>The artifact transform is not an integral part of downloading dependencies in this way. It so happened that the example I chose from my work used it. Setting up a custom repository is simpler if you don't use any transforms. Of course, this example was also achievable without transforms. I could have registered a task of type <code>Sync</code> and configured it to decompress the downloaded zip file, rather than doing it using a transform. You say potato, I say potato.</p>"},{"location":"writing/gradle/custom-repositories/#you-should-put-custom-repository-declarations-into-plugins-not-into-project-build-scripts","title":"You should put custom repository declarations into plugins, not into project build scripts","text":"<p>This code comes from a plugin I extracted which I called <code>grafana-trino-datasource-plugin</code>. All it does is create and populate this configuration. I didn't want this in the project build script because it's so noisy and verbose and distracts you as a reader from what matters: This project build knows how to download this directory from GitHub and use it.</p>"},{"location":"writing/gradle/custom-repositories/#appendix-the-complete-example","title":"Appendix - The complete example","text":"<p>This script plugin downloads a release artifact (a zip file) from a public GitHub repository and then transforms it into a directory for later use. In the real example, the directory is later added to a Docker image. For this page, I have added the <code>printDownloadedDirectoryPath</code> task to demonstrate how you might go about using the downloaded file in a task.</p> <pre><code>import org.gradle.api.artifacts.type.ArtifactTypeDefinition\nimport org.gradle.api.internal.artifacts.transform.UnzipTransform\nimport org.gradle.kotlin.dsl.creating\nimport org.gradle.kotlin.dsl.dependencies\nimport org.gradle.kotlin.dsl.getValue\nimport org.gradle.kotlin.dsl.registerTransform\nimport org.gradle.kotlin.dsl.repositories\n\nplugins {\n    base\n}\n\nrepositories {\n    exclusiveContent {\n        forRepository {\n            ivy {\n                url = uri(\"https://github.com/trinodb/grafana-trino\")\n                patternLayout { artifact(\"releases/download/v[revision]/[artifact]-[revision].[ext]\") }\n                metadataSources {\n                    artifact()\n                }\n            }\n        }\n        filter {\n            includeGroup(\"trinodb\")\n        }\n    }\n}\n\nval trinoDatasourcePlugin by configurations.creating {\n    isCanBeResolved = true\n    isCanBeConsumed = false\n    attributes.attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE)\n}\n\ndependencies {\n    registerTransform(UnzipTransform::class) {\n        from.attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.ZIP_TYPE)\n        to.attribute(ArtifactTypeDefinition.ARTIFACT_TYPE_ATTRIBUTE, ArtifactTypeDefinition.DIRECTORY_TYPE)\n    }\n\n    trinoDatasourcePlugin(\"trinodb:trino-datasource:1.0.6\") {\n        artifact {\n            type = \"zip\"\n            isTransitive = false\n        }\n    }\n}\n\ntasks.register(\"printDownloadedDirectoryPath\") {\n    val inputDir = configurations.named(\"trinoDatasourcePlugin\").map { it.singleFile }\n    doFirst {\n        logger.lifecycle(\"Found file: \" + inputDir.get().path)\n    }\n}\n</code></pre> <p>Created on 2024-05-24</p> <p>Updated on 2025-02-23</p>"},{"location":"writing/gradle/custom-task-types/","title":"Custom task types","text":"<p>This short article shows a custom task definition, and uses it to describe how in general to author a useful custom Gradle task, whose cacheability properties you understand.</p>"},{"location":"writing/gradle/custom-task-types/#the-example-task-class","title":"The example task class","text":"<p>Here's a custom task type definition:</p> <pre><code>@DisableCachingByDefault\nabstract class DockerBuild @Inject constructor(\n    objects: ObjectFactory,\n    private val execOps: ExecOperations,\n    private val layout: ProjectLayout\n) : DefaultTask() {\n\n    @get:InputFile\n    val dockerfile: RegularFileProperty = objects.fileProperty().convention(layout.projectDirectory.file(\"Dockerfile\"))\n\n    @get:InputFiles\n    abstract val resources: ConfigurableFileCollection\n\n    @get:Input\n    abstract val t: Property&lt;String&gt;\n\n    @TaskAction\n    fun build() {\n        val d = layout.buildDirectory.dir(name).get().asFile\n        d.mkdirs()\n        dockerfile.get().asFile.copyTo(d.resolve(\"Dockerfile\"), overwrite = true)\n        resources.forEach { it.copyRecursively(d.resolve(it.name), overwrite = true) }\n        execOps.exec {\n            commandLine(\"docker\", \"build\", d.absolutePath, \"-t\", t.get())\n        }.assertNormalExitValue()\n    }\n}\n</code></pre> <p>Take note of these several aspects this class.</p>"},{"location":"writing/gradle/custom-task-types/#it-is-abstract","title":"It is abstract","text":"<p>This is because Gradle is resposible for instantiating task instances at runtime, when they are needed.</p>"},{"location":"writing/gradle/custom-task-types/#it-has-a-constructor-annotated-with-inject","title":"It has a constructor annotated with <code>@Inject</code>","text":"<p>Since Gradle is responsible for instantiating instances of the task class, it also provides a way to provide useful build-logic-related objects to its instances. For example, the above class uses <code>ExecOperations</code> for running arbitray shell commands, and it uses <code>ProjectLayout</code> to get a convenient reference for the build directory.</p> <p>There are many other types that can be injected into a custom task class definition. Here are a few of the most useful ones:</p> <ul> <li><code>ExecOperations</code>: For running shell commands.</li> <li><code>ProjectLayout</code>: For convenient access to the project directory (i.e. the directory containing the <code>build.gradle(.kts)</code> file)</li> <li><code>FileSystemOperations</code>: Provides access to some Gradle-defined file-related methods, like <code>copy</code>, <code>sync</code>, and <code>delete</code>.</li> <li><code>ObjectFactory</code>: Can be used to instantiate various Gradle-defined types, such as the various kinds of <code>Property&lt;T&gt;</code> (<code>ListProperty&lt;T&gt;</code>, <code>MapProperty&lt;K, V&gt;</code>, <code>SetProperty&lt;T&gt;</code>, <code>RegularFileProperty</code>, <code>DirectoryProperty</code>), <code>ConfigurableFileCollection</code>, and generally any Gradle-registered type. This should only be used for creating these Gradle-specific types. In general, creating objects should be done by invoking a constructor.</li> </ul>"},{"location":"writing/gradle/custom-task-types/#it-extends-defaulttask","title":"It extends <code>DefaultTask</code>","text":"<p>This is the only Gradle-defined task type that I have ever extended in a custom task type definition. That's not to say that using other Gradle-defined tasks as a base is a bad idea, but certainly it's not something I would expect to do often. <code>DefaultTask</code> is unopinionated and I like that.</p>"},{"location":"writing/gradle/custom-task-types/#it-declares-its-default-cacheability","title":"It declares its default cacheability","text":"<p>This task is not cacheable by default. A subclass of this task type could be cacheable, and I could even make a task of this type cacheable by explicitly setting it to be cachable using <code>cacheIf { ... }</code> when registering it.</p> <p>If I wanted to absolutely preclude caching for this task, I would mark it <code>@UntrackedTask</code>. If I wanted it say it is cacheable, I'd mark it <code>@Cacheable</code>.</p> <p>Note however that a task cannot be cacheable unless it declares at least one input and output. An input is required because without it you cannot compare an imminent execution to previous executions (well, you could, but Gradle doesn't). An output is required because without it, what would be taken from the cache? The notion of cacheability doesn't make sense without an output. Of course, you may want to skip work done by a task which doesn't have any outputs per se. In such a case, you can create an empty text file (or perhaps a text file summarising the work that was done) to use as your cacheable \"output\".</p>"},{"location":"writing/gradle/custom-task-types/#its-fields-are-annotated-to-indicate-their-relevance-to-work-avoidance","title":"Its fields are annotated to indicate their relevance to work avoidance","text":"<p>For tasks declared as cacheable, Gradle avoids work by taking a fingerprint (i.e. a hash) of a task's inputs and comparing it to a hash of inputs from previous task executions. If a match is found between the current input hash and a previous execution's hash, then the task is said to have found a cache hit, the execution is skipped, and the declared output artifacts corresponding to the identical task input hash are restored to the file system, for consumption either by you, or by other tasks.</p> <p>This mechanism of work avoidance requires tasks to declare their inputs and outputs explicitly. This can be done either when registering a tasks (using the inputs.file(...) / outputs.file(...) methods and their friends), or within the definition of the task type, which I think is often better because it encapsulates the work avoidance role of certain configurable properties of the task.</p> <p>If a property bears no relevance to caching, it should be annotated with <code>@Internal</code>. Otherwise, it should have an annotation such as <code>@InputProperty</code>, <code>@InputFile</code>, <code>@InputFiles</code>, <code>@InputDirectory</code>, <code>@OutputFile</code>, <code>@OutputDirectory</code>. Note that a property unfortunately cannot be an output. The way to get around this in Gradle if you need this capability, is to put the value in a file and mark that file as an <code>@OutputFile</code>.</p>"},{"location":"writing/gradle/custom-task-types/#it-has-configurable-inputs","title":"It has configurable inputs","text":"<p>The field <code>t</code>, corresponding to the docker build parameter <code>-t</code>, is configurable. It has a  type of <code>Property&lt;String&gt;</code>. This means that when registering the task, the build author will need to set its value by using <code>t.set(...)</code>, passing a <code>String</code>.</p> <p>Note that for the <code>dockerfile</code> input, it has been instantiated to a property value directory using the <code>ObjectFactory</code>. This has been done so that the task class definition can include the conventional value for this input - a file in the project directory called \"Dockerfile\". This use of <code>ObjectFactory</code> to provide conventional values for properties can be done for both inputs and outputs.</p>"},{"location":"writing/gradle/custom-task-types/#it-has-a-method-annotated-with-taskaction","title":"It has a method annotated with <code>@TaskAction</code>","text":"<p>There should exactly one method in a task class definition annotated with <code>@TaskAction</code>, and this is the method that Gradle will call to execute the task, if it needs to be executed. It needs to accept no arguments. It should have a meaningful name.</p>"},{"location":"writing/gradle/custom-task-types/#further-notes","title":"Further notes","text":"<p>Custom task types should be declared within included-build plugins, as described in my longer article on structuring a Gradle build.</p> <p>Created on 2024-05-15</p> <p>Updated on 2024-08-30</p>"},{"location":"writing/gradle/gradle-monorepo-structure/","title":"Gradle monorepo structure","text":"<p>This tutorial will describe how I would generally go about structuring a Gradle monorepo project as someone who works at Gradle, and so knows the insider idioms of the tool fairly well.</p>"},{"location":"writing/gradle/gradle-monorepo-structure/#start-simple","title":"Start simple","text":"<p>The minimum viable Gradle build consists of a settings file, a single subproject and that subproject's build script. All the build logic is in Kotlin.</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % tree .\n.\n\u251c\u2500\u2500 gradle\n\u2502   \u2514\u2500\u2500 wrapper\n\u2502       \u251c\u2500\u2500 gradle-wrapper.jar\n\u2502       \u2514\u2500\u2500 gradle-wrapper.properties\n\u251c\u2500\u2500 gradlew\n\u251c\u2500\u2500 gradlew.bat\n\u251c\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 teashop.iml\n\u2514\u2500\u2500 teasite\n    \u2514\u2500\u2500 build.gradle.kts\n</code></pre> <pre><code>/////////////////////////////\n//// settings.gradle.kts ////\n/////////////////////////////\nrootProject.name = \"teashop\"\ninclude(\"teasite\")\n</code></pre> <pre><code>//////////////////////////////////\n//// teasite/build.gradle.kts ////\n//////////////////////////////////\nplugins {\n    base\n}\n</code></pre> <p>With this you can run e.g. <code>./gradlew :teasite:tasks --all</code></p> <p>The root project's build script is not included in my minimum viable Gradle build. You don't need it, and in fact I would avoid adding one because it encourages you and your team to do things that hurt your configuration time, in particular, using <code>allprojects</code> and <code>subprojects</code> blocks. This is called cross project configuration. {.is-info}</p> <p>A practical consideration when editing build logic is that you should be running IDE syncs fairly frequently, so that Intellij can keep its model of your project up to date with the changes you're making to the project's structure. If you have a large build with bad IDE sync times, then this can hurt. At the same time, it is possible to batch together several changes that affect IDE sync, and the sync them all at once. I don't advise doing this unless you're confident with what you're doing.</p>"},{"location":"writing/gradle/gradle-monorepo-structure/#more-projects-with-dependencies-between-each-other","title":"More projects with dependencies between each other","text":"<p>Add more projects by creating more directories under the root project directory, giving them build scripts, and including them in the root settings script. That's all there is to that. Something worth bearing in mind is that as you reach a high number of projects, your configuration time is going to increase. It can sometimes be appropriate to add additional source sets, rather than additional projects. I can also recommend using the jvm-test-suites plugin for creating multiple test source sets in a project.</p> <p>When you start to have many projects with dependencies on each other, there may come a point where you want to share something between projects that isn't a JAR. This sample in the docs gives a minimal example (sharing a single file) of the simplest way to do that.</p> <p>Gradle pros may encourage you to use something called variant-aware dependency selection and constrain your dependencies using attributes, rather than using the name of configurations directly. This is not hard to do, but unfortunately the docs on it are sparse and I don't want to get into that level of detail here. Perhaps I will submit a PR at some point to add that. {.is-info}</p>"},{"location":"writing/gradle/gradle-monorepo-structure/#custom-build-logic","title":"Custom build logic","text":"<p>My favourite thing about Gradle compared to other build tools is that you can program your build as though it were any other JVM-language program. Gradle provides a framework for modeling tasks, their dependencies and the artefacts they produce. The framework implements work avoidance, and provides a few common build task behaviours out-of-the-box. Since you can provide your model as code, it can benefit from flexibility and reuse in the same way as any other program you write, meaning that as your build gets larger and more complex, you can define ever more powerful abstractions for modeling your build.</p> <p>The documentation for this capability is unfortunately quite sparse. There are two main ways to do it, and the better way, which I will describe here, was introduced fairly recently.</p> <p>The best way to add custom build logic to your Gradle monorepo is to use included build plugins. That is, you have a directory underneath the root project, which is not a normal subproject. Instead, it is an included build. This directory contains its own settings script, to indicate that it is its own Gradle build, and like its parent, this is a multi-project build. The projects it contains define script Gradle plugins, which you can use to extract abstractions from project builds within its parent, in order to reuse them across many projects.</p> <p>The more well-known way to provide custom build logic in a Gradle build is to use buildSrc. This was the recommendation for a long time, but it has since become outdated. The reason for this is that any change you make to buildSrc invalidates up-to-date checks on every project in the build. As your build grows, this invalidation starts to become very expensive and makes it difficult to work productively on build logic.</p> <p>At even larger scale (i.e. literally thousands of subprojects), it can become economical to start publishing plugins to an internal repository and consuming them as external dependencies (i.e. using a buildscript block), rather than rebuilding the sources directly from within your monorepo. May you never experience that \ud83d\ude4f. {.is-info}</p> <p>Let's say I have a monorepo that contains two projects which both define AWS Lambda functions, <code>teasite</code> and <code>teapayments</code>:</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % tree .\n.\n\u251c\u2500\u2500 gradle\n\u2502   \u2514\u2500\u2500 wrapper\n\u2502       \u251c\u2500\u2500 gradle-wrapper.jar\n\u2502       \u2514\u2500\u2500 gradle-wrapper.properties\n\u251c\u2500\u2500 gradlew\n\u251c\u2500\u2500 gradlew.bat\n\u251c\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 teapayments\n\u2502   \u251c\u2500\u2500 build.gradle.kts\n\u2502   \u2514\u2500\u2500 src\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 teashop.iml\n\u2514\u2500\u2500 teasite\n    \u251c\u2500\u2500 build.gradle.kts\n    \u2514\u2500\u2500 src\n        \u2514\u2500\u2500 ...\n</code></pre> <p>For both projects, we have a Gradle task that packages the code and uploads it as a zip to AWS for use in the lambda. So for both projects, the build script is defining the same AWS dependencies, and the same custom task for uploading the lambda to AWS. The two project build scripts look almost identical. Here is one of them:</p> <pre><code>//////////////////////////////////\n//// teasite/build.gradle.kts ////\n//////////////////////////////////\nimport org.gradle.api.DefaultTask\nimport org.gradle.api.file.RegularFileProperty\nimport org.gradle.api.provider.Property\nimport org.gradle.api.tasks.*\nimport org.gradle.process.ExecOperations\nimport javax.inject.Inject\n\nplugins {\n  id(\"java\")\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n  ...\n}\n\n// Specify Amazon Corretto JDK 11\njava {\n    toolchain {\n        languageVersion.set(JavaLanguageVersion.of(11))\n        vendor.set(JvmVendorSpec.AMAZON)\n    }\n}\n\n// Definition of a task that uploads an AWS Lambda zip\n@UntrackedTask(because = \"We want to run this every time it is invoked\")\nabstract class UploadLambda @Inject constructor(private val execOps: ExecOperations) : DefaultTask() {\n\n    @get:Input\n    abstract val functionName: Property&lt;String&gt;\n\n    @get:InputFile\n    @get:PathSensitive(PathSensitivity.RELATIVE)\n    abstract val zip: RegularFileProperty\n\n    @TaskAction\n    fun upload() {\n        execOps.exec {\n            commandLine(\n                \"aws\", \"lambda\", \"update-function-code\",\n                \"--function-name\", functionName.get(),\n                \"--zip-file\", \"fileb://${zip.get().asFile.absolutePath}\"\n            )\n        }\n    }\n}\n\n// Task for putting the compiled Lambda's classes and dependencies into a zip of the right structure.\nval buildZip by tasks.registering(Zip::class) {\n    archiveBaseName.set(\"lambda\")\n    from(tasks.named&lt;JavaCompile&gt;(\"compileJava\").flatMap { it.destinationDirectory })\n    from(tasks.named&lt;ProcessResources&gt;(\"processResources\").map { it.destinationDir })\n    into(\"lib\") {\n        from(configurations.getByName(\"runtimeClasspath\"))\n    }\n}\n\n// Task for uploading zip to AWS\ntasks.register&lt;UploadLambda&gt;(\"uploadLambda\") {\n  functionName.set(\"teasite\")\n  zip.set(buildZip.flatMap { it.archiveFile })\n}\n</code></pre> <p>There are a few things going on here. There a toolchain definition, a custom task definition, and the registration of a couple of tasks. All of this is duplicated between the two projects in our build for no benefit, and you're not really thrilled about this.</p> <p>There is a way for us to elegantly extract appropriate abstractions and make both builds differ only in the way that matters. In this case, the projects differ in two dimensions: - They represent different Lambda functions - Their dependencies might be slightly different depending on what they do.</p> <p>The idiomatic way to extract common build logic is to use Gradle plugins. In this case, what we'd like is for both builds to declare that they are AWS Lambda functions by applying an appropriate plugin, and then configure some DSL that lets them specify the name of the function that they are. Below shows a nice possible outcome:</p> <pre><code>//////////////////////////////////////////////////\n//// Possible future teasite/build.gradle.kts ////\n//////////////////////////////////////////////////\nplugins {\n    id(\"teashop.aws-lambda\")\n}\n\nawsLambda {\n    functionName.set(\"teasite\")\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    ...\n}\n</code></pre>"},{"location":"writing/gradle/gradle-monorepo-structure/#creating-a-plugin-that-does-nothing","title":"Creating a plugin that does nothing","text":"<p>I think it's important to be able to take an arbitrary first step when working in unfamiliar terrain. In this section I'll show how to create a Gradle plugin in your monorepo that does pretty much nothing.</p> <p>First, create the included build for your monorepo's custom plugins, as described at the start of this subsection. For the teashop example, I'll make a directory under \"gradle\", called \"plugins\", which results in having this structure:</p> <pre><code>.\n\u251c\u2500\u2500 gradle\n\u2502   \u2514\u2500\u2500 wrapper\n\u2502       \u251c\u2500\u2500 gradle-wrapper.jar\n\u2502       \u2514\u2500\u2500 gradle-wrapper.properties\n\u2502   \u2514\u2500\u2500 plugins\n\u2502       \u2514\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 gradlew\n\u251c\u2500\u2500 gradlew.bat\n\u251c\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 teapayments\n\u2502   \u251c\u2500\u2500 build.gradle.kts\n\u2502   \u2514\u2500\u2500 src\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 teashop.iml\n\u2514\u2500\u2500 teasite\n    \u251c\u2500\u2500 build.gradle.kts\n    \u2514\u2500\u2500 src\n        \u2514\u2500\u2500 ...\n</code></pre> <pre><code>/////////////////////////////\n//// settings.gradle.kts ////\n/////////////////////////////\nrootProject.name = \"teashop\"\n\npluginManagement {\n    // Specifies an included build for defining custom plugins in the multi-project build.\n    includeBuild(\"gradle/plugins\")\n}\n\ninclude(\"teasite\")\ninclude(\"teapayments\")\n</code></pre> <pre><code>/////////////////////////////////////\n//// plugins/settings.gradle.kts ////\n/////////////////////////////////////\nrootProject.name = \"plugins\"\n</code></pre> <p>At this point, the teashop build specifies an included build called \"plugins\" from which to draw custom Gradle plugins. However, it currently doesn't contain any plugins. We will now add the \"no-op\" plugin: A plugin that does nothing.</p> <p>I'll first create an empty subproject in the included build for Gradle plugins:</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % tree plugins\nplugins\n\u251c\u2500\u2500 no-op\n\u2502   \u2514\u2500\u2500 build.gradle.kts\n\u2514\u2500\u2500 settings.gradle.kts\n</code></pre> <pre><code>/////////////////////////////////////\n//// plugins/settings.gradle.kts ////\n/////////////////////////////////////\nrootProject.name = \"plugins\"\ninclude(\":no-op\")\n</code></pre> <p>The next step is to turn the empty <code>no-op</code> project into a project that defines a Gradle plugin in Kotlin. This is not trivial, especially if you have never done it before. The first step is to apply the built-in <code>kotlin-dsl</code> Gradle plugin to the <code>no-op</code> project build:</p> <pre><code>////////////////////////////////////////\n//// gradle/plugins/no-op/build.gradle.kts ////\n////////////////////////////////////////\nplugins {\n    `kotlin-dsl`\n}\n\nrepositories {\n    mavenCentral()\n}\n</code></pre> <p>Next, I run an Intellij sync, wait for it to complete, and then use Intellij to generate the main Kotlin source root by hovering over the project in Intellij's Project panel, using 'New' via CMD-N (or CTRL-N on Linux/Windows), selecting 'Directory' and then 'src/main/kotlin'. Whether by this method or another, the resuting directory structure of the plugins included build should be this:</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % tree gradle/plugins \nplugins\n\u251c\u2500\u2500 no-op\n\u2502   \u251c\u2500\u2500 build.gradle.kts\n\u2502   \u2514\u2500\u2500 src\n\u2502       \u2514\u2500\u2500 main\n\u2502           \u2514\u2500\u2500 kotlin\n\u2514\u2500\u2500 settings.gradle.kts\n</code></pre> <p>The last step towards making an included build Gradle plugin is making a Gradle script under the newly created src/main/kotlin directory. The file's name needs to end with <code>.gradle.kts</code>. I have seen, and myself use the format: <code>&lt;org&gt;.&lt;plugin-name&gt;.gradle.kts</code>. In this example, I will create the file <code>teashop.no-op.gradle.kts</code>. Inside it, I'm going to put some unimportant build logic that proves that the plugin is being applied.</p> <pre><code>////////////////////////////////////////////////////////////////\n//// gradle/plugins/no-op/src/main/kotlin/teashop.no-op.gradle.kts ////\n////////////////////////////////////////////////////////////////\ntasks.register(\"sayHello\") {\n    doFirst {\n        logger.lifecycle(\"Hello!\")\n    }\n}\n</code></pre> <p>Without doing anything else, this plugin can now be applied by the projects of the build. For example:</p> <pre><code>//////////////////////////////////////\n//// teapayments/build.gradle.kts ////\n//////////////////////////////////////\nplugins {\n  id(\"teashop.no-op\")\n}\n</code></pre> <p>Which allows me to do this:</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % ./gradlew :teapay:sayHello\n\n&gt; Task :teapayments:sayHello\nHello!\n\nBUILD SUCCESSFUL in 731ms\n11 actionable tasks: 1 executed, 10 up-to-date\n</code></pre>"},{"location":"writing/gradle/gradle-monorepo-structure/#creating-a-plugin-that-does-something","title":"Creating a plugin that does something","text":"<p>As is sometimes the case when going from an arbitrary first step to a meaningful follow-up step, the hard work is already done. From where we are with the <code>no-op</code> plugin, we need only to rename it and add some useful Gradle build configuration to the plugin, which is presently defined in <code>gradle/plugins/no-op/src/main/kotlin/teashop.no-op.gradle.kts</code>. Whatever Gradle code we add to that script will be ran for any project which applies the plugin. This includes any plugins that are specified for inclusion in the plugin. A plugin can even simply proxy other plugin(s), by containing only a <code>plugins { ... }</code> block.</p> <p>In the AWS Lambda example above, there was a class definition inlined into the project build. When pulling that code into a plugin, I would not leave the class definition in the script, I'd move it to its own file. To do that, create a package underneath the Kotlin source root of the plugin, and define classes in it. You'll need to import those classes into the plugin script in order to use them. I'll show an example by refactoring the <code>teashop.no-op</code> plugin:</p> <pre><code>///////////////////////////////////////////////////////////////////\n//// gradle/plugins/no-op/src/main/kotlin/com/teashop/SayHelloTask.kt ////\n///////////////////////////////////////////////////////////////////\npackage com.teashop\n\nimport org.gradle.api.DefaultTask\nimport org.gradle.api.tasks.TaskAction\nimport org.gradle.work.DisableCachingByDefault\n\n@DisableCachingByDefault(because = \"By default you should say hello.\")\nabstract class SayHelloTask : DefaultTask() {\n\n    @TaskAction\n    fun sayHello() {\n        logger.lifecycle(\"Hello!\")\n    }\n}\n</code></pre> <pre><code>////////////////////////////////////////////////////////////////\n//// gradle/plugins/no-op/src/main/kotlin/teashop.no-op.gradle.kts ////\n////////////////////////////////////////////////////////////////\nimport com.teashop.SayHelloTask\n\ntasks.register&lt;SayHelloTask&gt;(\"sayHello\")\n</code></pre> <p>As before:</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % ./gradlew :teapay:sayHello\n\n&gt; Task :teapayments:sayHello\nHello!\n\nBUILD SUCCESSFUL in 1s\n11 actionable tasks: 4 executed, 7 up-to-date\n</code></pre> <p>The final project layout is this:</p> <pre><code>rob@Robs-MacBook-Pro-2 teashop % tree .                    \n.\n\u251c\u2500\u2500 gradle\n\u2502   \u2514\u2500\u2500 wrapper\n\u2502       \u251c\u2500\u2500 gradle-wrapper.jar\n\u2502       \u2514\u2500\u2500 gradle-wrapper.properties\n\u2502   \u2514\u2500\u2500 plugins\n\u2502       \u251c\u2500\u2500 no-op\n\u2502       \u2502   \u251c\u2500\u2500 build.gradle.kts\n\u2502       \u2502   \u2514\u2500\u2500 src\n\u2502       \u2502       \u2514\u2500\u2500 main\n\u2502       \u2502           \u2514\u2500\u2500 kotlin\n\u2502       \u2502               \u251c\u2500\u2500 com\n\u2502       \u2502               \u2502   \u2514\u2500\u2500 teashop\n\u2502       \u2502               \u2502       \u2514\u2500\u2500 SayHelloTask.kt\n\u2502       \u2502               \u2514\u2500\u2500 teashop.no-op.gradle.kts\n\u2502       \u2514\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 gradlew\n\u251c\u2500\u2500 gradlew.bat\n\u2502   \u2514\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 settings.gradle.kts\n\u251c\u2500\u2500 teapayments\n\u2502   \u251c\u2500\u2500 build.gradle.kts\n\u2502   \u2514\u2500\u2500 src\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 teashop.iml\n\u2514\u2500\u2500 teasite\n    \u251c\u2500\u2500 build.gradle.kts\n    \u2514\u2500\u2500 src\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"writing/gradle/gradle-monorepo-structure/#patterns-for-plugins","title":"Patterns for plugins","text":"<p>Sometimes, a plugin is highly reusable and flexible. Other times, a plugin may be quite specific to one particular project build. Sometimes, a plugin registers tasks itself and the applying project doesn't need to do anything. Other times, it may be more appropriate for the plugin to instead add custom types onto the classpath of the applying project, so that the applying project can register and configure tasks for itself. Do whatever works in your context.</p> <p>One of the idiomatic ways to organise Gradle plugins is to describe what a project is. For example, when applying the <code>teashop.aws-lambda</code> plugin to the <code>teasite</code> project, you might read that as \"The teasite project is an AWS Lambda project\". Of course, many built-in plugins don't conform to that idiom, so I don't feel at all constrained by it.</p> <p>A pattern I often use is to define a script plugin that is completely empty, but defines useful classes and Kotlin functions that are added to the classpath of a applying project's build script. For example, if my <code>teashop.no-op</code> plugin was totally empty, I would still have been able to register tasks of type <code>SayHelloTask</code> within the teasite project, since it applies the no-op plugin.</p> <p>Similarly, you can define a completely empty script plugin which brings along extension methods for existing Gradle types and puts them on your project builds' classpath. With this pattern it becomes possible to extend the Gradle DSL to provide abstractions of your choosing.</p>"},{"location":"writing/gradle/gradle-monorepo-structure/#summary","title":"Summary","text":"<p>So here are the elements that I think are important for the ideal, idiomatic layout of a monorepo whose build tool is Gradle:</p> <ul> <li>A multi-project build with no root build script.</li> <li>Shared build logic provided by plugins defined in an included build.</li> </ul>"},{"location":"writing/gradle/gradle-monorepo-structure/#links-and-further-reading","title":"Links and further reading","text":"<ul> <li>A worked example of this structure, made by my dad by following this tutorial: https://github.com/ivanmoore/gradle-monorepo-setup</li> <li>Gradle source: https://github.com/gradle/gradle</li> <li>Gradle docs: https://docs.gradle.org</li> <li>Gradle site: https://gradle.org</li> <li>Part 1 of a 2-part series on Square's build woes: https://developer.squareup.com/blog/herding-elephants/</li> <li>Part 2 of a 2-part series on Square's build woes: https://developer.squareup.com/blog/stampeding-elephants</li> <li>A cute little trick you can do with Gradle (although it would be better if supplied by a plugin, and with tests): https://jonnyzzz.com/blog/2016/03/06/gradle-all-maven-runner/</li> <li>What I'm working on at Gradle: https://gradle.com</li> </ul> <p>Created on 2023-05-07</p> <p>Updated on 2025-02-23</p>"},{"location":"writing/gradle/insider-guidelines/","title":"Insider guidelines","text":"<p>These are some things I've learned since working at Gradle, that I don't know for sure I would have ever learned from just reading the docs as a Java developer anywhere else.</p>"},{"location":"writing/gradle/insider-guidelines/#use-the-kotlin-dsl","title":"Use the Kotlin DSL","text":"<p>As much as the magic and mystery of Groovy is fun and exotic, most of us want to have a standard, tool-assisted programming experience.</p> <p>The trade-off is that Kotlin requires compilation, and therefore is slower. For a very large build, Groovy DSL scripts will result in a faster overall build. You probably don't have that problem. Your problem is probably that you don't understand your build, or your build is incorrectly defined and you have to frequently run <code>clean</code> to deal with incorrectly defined inputs and outputs in misconfigured tasks from third party plugins that are doing insane things. Your problem is possibly also that your build does a lot of unnecessary work because you aren't taking advantage of the features of Gradle that enable your build to be much faster than it is. In the big, prioritised list of \"things that could speed up your build\", I suspect (although this is just my uninformed opinion) that using Groovy DSL realistically belongs quite low on that list.</p>"},{"location":"writing/gradle/insider-guidelines/#dont-have-a-root-project-build","title":"Don't have a root project build","text":"<p>At the root of your Gradle build, you need a <code>settings.gradle(.kts)</code>. Don't have <code>build.gradle(.kts)</code> at this level though, because it ends up being a gravity well for all sorts of nonsense that you don't want, like <code>subprojects { ... }</code> and <code>allproject { ... }</code> blocks.</p>"},{"location":"writing/gradle/insider-guidelines/#dont-use-subprojects-or-allprojects-blocks","title":"Don't use <code>subprojects {}</code> or <code>allprojects {}</code> blocks","text":"<p>These constructions defeat optimizations that I am not smart enough to understand, but which are pests in a way that I do understand, which is that they provide poor locality for build logic. They enable the root project build to influence the build logic of all other projects in a multi-project build in ways that are difficult to reason about. That property is probably related to why these blocks are bad for performance, although I'm not completeley sure.</p>"},{"location":"writing/gradle/insider-guidelines/#running-clean-should-only-be-done-if-you-have-a-serious-bug-in-your-build","title":"Running <code>clean</code> should only be done if you have a serious bug in your build","text":"<p>I pretty much never use the <code>clean</code> task. To habitually run it is a bad habit that is probably hurting your development experience, unless you are working within a build that is broken. If you live with such a broken build, you should get on top of that because it is really hurting your build time.</p> <p>Your nuclear option is <code>git clean -fdx</code>. If this doesn't fix the problem then God help you.</p>"},{"location":"writing/gradle/insider-guidelines/#if-you-are-editing-build-logic-you-need-to-run-idea-sync-often","title":"If you are editing build-logic, you need to run IDEA sync often","text":"<p>I often have the following development flow: Apply a new plugin to a project build script =&gt; immediately run IDEA sync =&gt; start using the types and Gradle objects defined by the plugin I just applied. If I didn't run that sync, I wouldn't be able to use those types in the IDE without it shouting in red at me. For builds with long syncs, this is a painful experience.</p> <p>I have written a dedicated page for some things I've learned about using the Kotlin DSL.</p>"},{"location":"writing/gradle/insider-guidelines/#dont-use-buildsrc-use-included-build-plugins","title":"Don't use buildSrc, use included-build plugins","text":"<p>Like a root project build script, buildSrc can quickly become a dumping ground for all sorts of things. Much worse, it is terrible for your build's performance, because every single change to buildSrc invalidates work avoidance for subsequent builds. On the Develocity project, this was a terrible toll for a long time, until the wizards of the developer productivity team initiated and sustained a migration to included-build plugins, taking many months and consuming much effort. Fortunately, this is the kind of migration project which yields progressive benefits: Things get slowly better as more is migrated to included-build plugins.</p> <p>The reason included-build plugins are better than buildSrc, and indeed another reason why using plugins is better than using <code>subprojects</code> and <code>allprojects</code>, is that when a central, global source of build logic changes, such as buildSrc, it forces recompilation for all project builds. For a project with many modules, this is massively expensive: All developers and CI machines must do this. If these sources don't exist on the other hand, a change in a single included-build plugin forces the recompilation only of those modules that transitively apply it - much less expensive!</p> <p>You can of course define Gradle plugins in buildSrc, and this is definitely much better in terms of build logic organisation and performance than using <code>subprojects</code> or <code>allprojects</code>, but it is worse than defining plugins in an included build, because changes in buildSrc plugins invalidate more projects than just those that apply the changed plugin.</p>"},{"location":"writing/gradle/insider-guidelines/#when-sharing-task-outputs-across-projects-use-attribute-matching","title":"When sharing task outputs across projects, use attribute matching","text":"<p>There are two safe ways to share task outputs across projects. I say \"safe\" because of course you can always just pass the correct output path to a consuming task and cross your fingers that Gradle schedules the producing task before the consuming task (note: that is not a good idea at all).</p> <p>The two safe ways are: - Declaring a dependency on another project's exported configuration by name (i.e. using the name of the configuration within the producing project) - Declaring a dependency on another project, and declaring a common attribute in both the exported (i.e. consumable i.e. producer) configuration and the imported (i.e. resolvable i.e. consumer) configuration.</p> <p>Both of these ways are demonstrated in a working example in my gradle-share-outputs-between-projects GitHub repository.</p> <p>Favour attribute matching over referencing configurations by name, because using attribute matching makes it more convenient to tightly couple references to the same exported configuration, by using shared code (i.e. in a plugin) for attribute values. Tightly coupled references to the same thing are desirable.</p>"},{"location":"writing/gradle/insider-guidelines/#use-java-toolchains-to-manage-target-jvm-versions","title":"Use Java Toolchains to manage target JVM versions","text":"<p>Understand and use Java Toolchains. This will spare you significant grief, as long as you understand what it is doing and what it is not doing.</p> <p>The version of Java used to invoke Gradle is not necessarily the same as the version of Java used to compile your Java code or run your tests. To control the version of Java used to compile your Java code, use Gradle's Java Toolchains feature. If you don't do this, Gradle will just use whatever JVM is being used to run itself when compiling and running Java code. Unless you have a good (and I imagine really quite interesting and unique) reason to preseve that behaviour, you should always be using Java Toolchains. I find the official documentation for this feature to be pretty useful and descriptive.</p>"},{"location":"writing/gradle/insider-guidelines/#the-info-and-debug-log-levels-for-gradle-are-mostly-not-very-useful-and-they-probably-wont-help-you-but-stacktrace-is-often-helpful","title":"The info and debug log levels for Gradle are mostly not very useful and they probably won't help you, but --stacktrace is often helpful","text":"<p>When Gradle fails, it suggests you may want to rerun using <code>--info</code> or <code>--debug</code>. In my experience this is mostly just useless noise. Using <code>--stacktrace</code> however is often quite helpful to isolate the line of build logic code that is the cause of a build failure. I use it quite often.</p>"},{"location":"writing/gradle/insider-guidelines/#make-intellij-idea-download-the-javadocs-and-sources-for-your-third-party-dependencies","title":"Make Intellij IDEA download the javadocs and sources for your third-party dependencies","text":"<p>It is enormously helpful to be able to navigate and inspect third-party code as if you had the sources for the version you're using checked out and available right in your IDE.</p> <p>You can achieve this experience by applying the following build logic to your projects:</p> <pre><code>plugins {\n    // other plugins\n    idea\n}\n\nidea {\n    module {\n        isDownloadSources = true\n        isDownloadJavadoc = true\n    }\n}\n\n// other build logic\n</code></pre> <p>Created on 2025-02-23</p> <p>Updated on 2024-05-15</p>"},{"location":"writing/gradle/intellij-idea/","title":"Intellij IDEA","text":"<p>Introduction: Using Gradle from Intellij IDEA</p> <p>Intellij IDEA is a big, complex, powerful tool, solving a problem that is harder than it looks from the outside. Gradle is similar in this way. Put them together, and you have a recipe for a horrible, overcomplicated experience. There are a few things you can do to have a better time.</p> <p>The page describes a few pitfalls that you should avoid, and virtues that you should strive towards. I update it opportunistically whenever I see something that makes me think everyone should know this. In that regard, it is similar to my page on Insider Tips.</p>"},{"location":"writing/gradle/intellij-idea/#intellij-says-unresolved-reference","title":"Intellij says \"Unresolved reference\"","text":"<p>In a Gradle project, this problem should be solved by a combination of adjusting the build, and synchronizing Intellij. The instructions you need to issue to Intellij may be different in different cases.</p> <p>Here are some items in the knowledge toolbox I use to solve this problem:</p>"},{"location":"writing/gradle/intellij-idea/#avoid-using-api-dependencies-in-gradle","title":"Avoid using \"api\" dependencies in Gradle.","text":"<p>These can be replaced with \"implementation\" dependencies, which lack the pitfalls and carry less risk of introducing wasteful, unused dependencies between projects, which slow down your build.</p>"},{"location":"writing/gradle/intellij-idea/#do-not-use-the-intellij-tooltip-action-add-foo-to-classpath","title":"Do not use the Intellij tooltip action \"Add 'foo' to classpath\"","text":"<p>This is nonsense, unless Intellij is going to edit your Gradle build files. When Intellij takes this option, what it is really doing is modifying its own module dependency representation and ignoring the one that it ordinarily recieves from Gradle. You should aim to update the Gradle build so that it supplies the desired dependency information to the IDE, rather than tricking the IDE into believing that the build is defining dependencies in the way you want, when in truth it isn't.</p>"},{"location":"writing/gradle/intellij-idea/#triggering-an-ide-sync-is-often-insufficient-you-must-also-run-repair-ide","title":"Triggering an IDE sync is often insufficient, you must also run \"Repair IDE\"","text":"<p>Once you have made a change to the build definition that you think might fix your dependency situation, you need to then:</p> <ol> <li>Trigger an IDE sync. I do this so often that I have it bound to a custom shortcut. I use Cmd-Shift-7.</li> <li>Trigger \"Repair IDE\". Again, I have this on a custom shortcut. I use Cmd-Ctrl-I (I as in IKEA).</li> </ol> <p>I like the \"Repair IDE\" flow because it offers you escalations after each attempt. If you run it initially and it doesn't fix your problem, when you think it should, then it will offer to \"Rescan Project Indexes\" as a next step. As the next step after that, I think it asks you to restart IDEA. I rarely have to go further than the first step, which I think just reloads all build files and project files from disk, although I don't actually know.</p> <p>Created on 2024-06-06</p> <p>Updated on 2024-06-06</p>"},{"location":"writing/gradle/jvms/","title":"JVMs","text":"<p>This page contains tips regarding the use of Gradle and different JVM versions.</p>"},{"location":"writing/gradle/jvms/#java-toolchains","title":"Java Toolchains","text":"<p>Gradle is a Java program. When you invoke Gradle (e.g. using <code>./gradlew</code>), you are starting one or more Java processes (you can check your locally running Java processes using a command line tool suhc as <code>jps</code>).</p> <p>Modern Gradle is compatible with Java 8 onwards. Running with the second most recent LTS Java release will give you the best experience, with the lowest likelihood of encountering bugs.</p> <p>Gradle will run using whatever version of Java is specified in the <code>JAVA_HOME</code> environment variable passed to the process that invokes it. - If you invoke Gradle from the command line using the wrapper (i.e. <code>./gradlew</code>), then your \"default\" <code>JAVA_HOME</code> value will be used. You can check this by running <code>echo $JAVA_HOME</code> from the same command line interface, or by running <code>env</code> and having a look through. - If you invoke Gradle through your IDE (e.g. Intellij) then the JAVA_HOME environment variable passed to the Gradle process is controlled by the IDE. There should be a setting somewhere to configure this. There certainly is in Intellij IDEA.</p> <p>The version of Java used to invoke Gradle is not necessarily the same as the version of Java used to compile your Java code or run your tests.</p> <p>If you don't use the Java Toolchains feature to specify the version of the JVM to use when compiling code or running tests, then Gradle will just use whatever JVM is being used to run itself. Unless you have a good (and I imagine really quite interesting and unique) reason to preseve that behaviour, you should always be using Java Toolchains to explicitly specify the version of the JVM to use for compiling code and running tests. I find the official documentation for this feature to be pretty useful and descriptive.</p> <p>Created on 2024-06-06</p> <p>Updated on 2024-06-06</p>"},{"location":"writing/gradle/kotlin-dsl/","title":"Kotlin DSL","text":""},{"location":"writing/gradle/kotlin-dsl/#code-completion-and-type-correctness-depends-on-an-up-to-date-ide-sync","title":"Code completion and type correctness depends on an up to date IDE sync","text":"<p>I run an IDE sync after almost every change to a plugin or build script if I am making subsequent changes that rely on it. This is because IDE auto-complete and type checking depend on having an updated classpath, which changes whenever you apply a new plugin or change some code in an applied plugin. It is annoying that it doesn't stay up-to-date automatically, as it does in application code. There is probably a reason for this that I don't understand.</p> <p>I have a custom shortcut in my Intellij to trigger an IDE sync (I use cmd-shift-7).</p>"},{"location":"writing/gradle/kotlin-dsl/#the-kotlin-plugins-all-come-from-maven-central","title":"The kotlin plugins all come from Maven Central","text":"<p>In order to use any kotlin plugin, you need this:</p> <pre><code>repositories {\n  mavenCentral()\n}\n</code></pre>"},{"location":"writing/gradle/kotlin-dsl/#you-cant-use-plugin-accessors-outside-of-project-build-script","title":"You can't use plugin accessors outside of project build script","text":"<p>If you try to do this inside a kotlin-dsl plugin (i.e. in buildSrc or in an included build plugin), you might get this incorrect, unhelpful suggestion from Intellij:</p> <p></p> <p>If you are offered an import of the form <code>gradle.kotlin.dsl.accessors._&lt;some hash&gt;</code> - don't take it. Instead what you need to do is use <code>the&lt;T&gt;()</code> or <code>extensions.getByName&lt;T&gt;(String extensionName)</code> to get the Gradle object you're looking for. The extension name is the same as the name that you would use in a project build script at the top level. For example if in a project build script you would write <code>dockerApplications { ... }</code>, then you are configuring an extension called \"dockerApplications\". To get the type, I use IDEA's quick documentation shortcut (ctrl-J).</p> <p>So in the image above where I am trying to access an extension called \"dockerApplications\" in my plugin, what I need to do is first define it within the plugin:</p> <pre><code>val dockerApplications = the&lt;DockerApplicationExtension&gt;()\n</code></pre> <p>Sometimes, the type will be more convoluted. For example, it could also have been this:</p> <pre><code>val dockerApplications = the&lt;NamedDomainObjectContainer&lt;DockerApplication&gt;&gt;()\n</code></pre> <p>Created on 2024-05-24</p> <p>Updated on 2024-05-24</p>"},{"location":"writing/gradle/registering-gradle-tasks/","title":"Registering Gradle tasks","text":"<p>The official documentation on authoring tasks starts here: https://docs.gradle.org/current/userguide/more_about_tasks.html.</p>"},{"location":"writing/gradle/registering-gradle-tasks/#simple-project-builds-dont-need-to-register-tasks","title":"Simple project builds don't need to register tasks","text":"<p>If you're like most Gradle users, you are probably writing some Java application, and using Gradle to create artifacts and run tests.</p> <p>These most common journeys are supported by tasks which are added by Gradle plugins you have applied e.g. the <code>java</code> plugin adds the <code>compileJava</code> task.</p> <p>Using these built-in tasks from plugins is good, because it means that you can benefit from improvements to the implementation of these tasks without putting in any effort yourself.</p> <p>You may have to configure these built-in tasks slightly, for example, you may have your source code in an unusual, different place, in which case you need to configure the <code>compileJava</code> task to compile the sources in that directory.</p> <p>Configuring tasks in this way is also good. You still take advantage of improvements to the task implementation which happen over time, and you don't need to do anything, which is ideal. If you are configuring a task in an unusual way though, you run the risk of having difficulty in upgrading your plugin version later, if the plugin's public interface changes in an incompatible way.</p>"},{"location":"writing/gradle/registering-gradle-tasks/#when-might-you-need-to-register-tasks","title":"When might you need to register tasks","text":""},{"location":"writing/gradle/registering-gradle-tasks/#you-are-writing-a-one-off-task","title":"You are writing a one-off task","text":"<p>In your build, you might need to do one little thing e.g. run some shell script. You can do this easily in Gradle by writing an <code>Exec</code> task, or even a <code>DefaultTask</code> that uses <code>project.exec</code>.</p> <p>For example:</p> <pre><code># my-app/build.gradle.kts\nplugins {\n  java\n}\n\n# Runs the script my-app/foo.sh using bash.\ntasks.register&lt;Exec&gt;(\"runFoo\") {\n  commandLine(listOf(\"bash\", \"foo.sh\"))\n}\n</code></pre> <p>Alternatively, you may need to do a custom thing. You might be tempted to do this:</p> <pre><code># my-app/build.gradle.kts\nplugins {\n  java\n}\n\ntasks.register&lt;MyCustomTaskType&gt;(\"doThing\")\n\nopen class MyCustomTaskType : DefaultTask() {\n  @TaskAction\n  fun doThing() {\n    ...\n  }\n}\n</code></pre> <p>I have come to dislike this and pretty much never do it. I don't place any overhead cost on defining a custom plugin even if just to to hold a custom task type and nothing else. It improves the declarativity of the project build script, making it easier for me to read and understand at a glance, and also puts the task type in its own source set in the project, where I can, for example, write tests for it, or factor it into multiple different objects, perhaps some completely separated from Gradle. It also opens the door more clearly for re-use in other projects.</p>"},{"location":"writing/gradle/registering-gradle-tasks/#you-are-writing-a-plugin","title":"You are writing a plugin","text":"<p>Sometimes you want your build to do something that is not provided by any plugin you can find, or the plugins that are available are rubbish, or you can't use them for whatever reason.</p> <p>In this scenario, you have reached the point where you may need to author your own plugin, and therefore author the corresponding tasks.</p> <pre><code># my-app/build.gradle.kts\nplugins {\n  java\n\n  # My custom Docker plugin\n  id(\"com.mycompany.docker\")\n}\n</code></pre> <p>For example, there are many publicly available Gradle plugins for Docker out there, but as of this writing (early 2024) none of them are sufficiently good or general purpose enough that I would choose them when I want to work with Docker images in my Gradle build, which I often do.</p>"},{"location":"writing/gradle/registering-gradle-tasks/#library-plugins","title":"Library plugins","text":"<p>A library plugin is a plugin created specifically to hold custom task definitions. This is a fairly uncommon pattern in the community of Gradle users, but it's one a really like and would like to proselytize among all Gradle users.</p> <p>Traditional Gradle plugins work like frameworks. The plugin defines everything and whrn a project build applies that plugin, they pick up everything as defined already by the plugin. The project build might configure a few things, but in general the plugin is quite rigid in the interface that it offers.</p> <p>The kind of plugins I like to write, which I refer to as 'library plugins' are different. The plugin configures little-to-no build logic, but the application of the plugin is expected to bring in a sweeping cast of helpful Gradle constructs, especially task definitions.</p>"},{"location":"writing/gradle/registering-gradle-tasks/#where-to-define-new-tasks","title":"Where to define new tasks","text":"<p>When you author your own tasks, you can choose to write them within project build scripts (i.e. in my-app/build.gradle.kts), or within custom plugins. The most accessible approach is to write all your build logic in your project build, because you don't need to know anything about build logic organisation to do that - you just dump your code into the project build script. Sometimes this is fine. It's a matter of taste and judgement, which you can pick up with a small amount of practice of using both locations for custom task definitions.</p> <p>In general, I avoid defining new task types (i.e. new classes which extend DefaultTask) within a project build, because doing so is ugly and unidiomatic. I mentioned this above too. A project build script should ideally contain only the application of plugins, configuration of build objects they define, and perhaps the creation of a handful of small bespoke tasks.</p> <p>If I'm defining a task that is specific to the project, then I register the task in the project build script. If the task is generic, or I know or anticipate that abstracting it into a plugin would be useful, then I would define the new task in a plugin that is applied. The task is then registered by the application of the plugin. If the task needs to be configured for each project that applies it, then the project build script can do that.</p> <p>To use an analogy of application programming in an object-oriented language, if I'm writing behaviour that is truly specific to the implementation of a specific class, it makes sense to capture that behaviour in a private method of that class. If however the implementation is a special case of a more generic behaviour that should be abstracted for re-use, I would at least consider extracting it into another class. To return from the analogy of application programming back to Gradle build development, if I want to add a task that is truly specific to a particular project's role in the build, then it makes sense to register it within the project build script. On the other hand if the task I want to add is actually a special case of something more generic that exists in other projects (and perhaps considering Kent Beck's Rule of Three), then I might prefer to define the task in a plugin which is applied to the project instead, and have the project configure the task in a special way if necessary.</p> <p>Created on 2023-12-24</p> <p>Updated on 2024-05-15</p>"},{"location":"writing/gradle/secure-dependencies/","title":"WIP: Secure Dependencies","text":"<p>This page describes how to use Gradle to deal with vulnerable dependencies in applications.</p>"},{"location":"writing/gradle/secure-dependencies/#a-direct-dependency-used-in-a-program-that-you-build-needs-to-be-updated-to-a-newer-version","title":"A direct dependency used in a program that you build needs to be updated to a newer version","text":"<p>If your build uses an old version of a dependency and that version has some CVEs against its name, you need to update the version declared in the build so that your program doesn't use it. This is the easiest and most straight forward case to fix.</p>"},{"location":"writing/gradle/secure-dependencies/#minimal-example-build","title":"Minimal example build","text":"<p>A JAR built by this build (using the <code>shadowJar</code> task) will contain a vulnerable version of guava, which will result in failed security scans for your JAR and any container image that contains this JAR.</p> <pre><code>plugins {\n    java\n    application\n    id(\"com.gradleup.shadow\") version \"8.3.4\"\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation(\"com.google.guava:guava:31.1-jre\")\n}\n\napplication {\n    mainClass.set(\"com.rrm.app.Main\")\n}\n</code></pre>"},{"location":"writing/gradle/secure-dependencies/#fix","title":"Fix","text":"<p>You need to update the declaration of this direct dependency in my build to use a newer version.</p> <pre><code>dependencies {\n    implementation(\"com.google.guava:guava:33.3.1-jre\")\n}\n</code></pre>"},{"location":"writing/gradle/secure-dependencies/#a-transitive-dependency-used-in-a-program-that-you-build-needs-to-be-updated-to-a-newer-version","title":"A transitive dependency used in a program that you build needs to be updated to a newer version","text":""},{"location":"writing/gradle/secure-dependencies/#minimal-example-build_1","title":"Minimal example build","text":"<p>This build depends on the classes from the above build. This means that transitively, it includes that vulnerable version of guava. We need to amend the build so that this build instead resolves a newer version of guava chosen by us.</p> <pre><code>plugins {\n    java\n    application\n    id(\"com.gradleup.shadow\") version \"8.3.4\"\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation(project(\":direct\"))\n}\n\napplication {\n    mainClass.set(\"com.rrm.transitive.Main\")\n}\n</code></pre>"},{"location":"writing/gradle/secure-dependencies/#fix_1","title":"Fix","text":"<p>To fix this, you need to tell Gradle that when it is resolving a dependency on guava, it should always use a version specified by you, that does not have any vulnerabilities. You can do this by appending the below snippet to the project build script. More commonly, you can define a plugin within your build that specifies this for all projects.</p> <pre><code>configurations.all {\n    resolutionStrategy {\n        eachDependency {\n            if (requested.group == \"com.google.guava\" &amp;&amp; requested.name == \"guava\") {\n                useVersion(\"33.3.1-jre\")\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"writing/gradle/secure-dependencies/#a-jar-included-in-a-container-image-you-build-needs-to-be-replaced-by-a-newer-version-of-the-same-jar","title":"A JAR included in a container image you build needs to be replaced by a newer version of the same JAR","text":""},{"location":"writing/gradle/secure-dependencies/#my-use-case-securing-apache-hive","title":"My use case: Securing Apache Hive","text":"<p>My real use case here is in building a custom variant of the Apache Hive image that removes all its security vulnerabilities.</p> <p>Apache Hive is borderline abandonware, but somehow is also an important element in data lakehouse applications using distributed query engines like Trino. If you want to build a query engine for unstructured data, on-prem (rather than using a managed service like AWS Glue), then you may end up faced with the prospect of running Apache Hive.</p> <p>When I first ran the Hive image through our image scanner of choice, Trivy, it came up with literally hundreds of vulnerabilities, including a very large number of critical vulnerabilities such as RCEs. If you are an on-prem software vendor, as my employer is, this is a showstopper, because your biggest and most important customers will struggle to onboard your product.</p> <p>For this page though and for the purposes of this example, I can't be bothered to replicate the nonsense I have done at work. Instead, I will demonstrate what is possible using some of Gradle's most fundemental but also least understood constructions: Configurations and Task types which manipulate Files and FileCollections.</p>"},{"location":"writing/gradle/secure-dependencies/#downloading-a-single-jar-and-putting-it-somewhere","title":"Downloading a single JAR and putting it somewhere","text":"<p>This build defines a single task, <code>putGuavaJarSomewhere</code>, which downloads the latest version of the guava JAR and puts it into the directory <code>build/somewhere</code>.</p> <p>When I needed to overwrite a JAR baked into the Apache Hive image, I did something like this.</p> <pre><code>plugins {\n    base\n}\n\nrepositories {\n    mavenCentral()\n}\n\nval guava by configurations.creating\n\ndependencies {\n    guava(\"com.google.guava:guava:33.3.1-jre\")\n}\n\ntasks {\n    val putGuavaJarSomewhere by registering(Sync::class) {\n        from(guava)\n        into(layout.buildDirectory.dir(\"somewhere\"))\n    }\n}\n</code></pre>"},{"location":"writing/gradle/secure-dependencies/#a-jar-included-in-an-image-you-build-has-shaded-classes-of-transitive-dependencies-that-need-to-be-replaced-by-classes-from-a-newer-version-of-the-same-transitive-dependency","title":"A JAR included in an image you build has shaded classes of transitive dependencies that need to be replaced by classes from a newer version of the same transitive dependency","text":"<p>TODO</p>"},{"location":"writing/gradle/secure-dependencies/#minimal-example-build_2","title":"Minimal example build","text":""},{"location":"writing/gradle/secure-dependencies/#fix_2","title":"Fix","text":""},{"location":"writing/gradle/secure-dependencies/#a-note-on-automatic-tests","title":"A note on automatic tests","text":"<p>In the last two cases, we are modifying third-party software after it has been released. This means the quality control that they may have done prior to shipping the software no longer applies to what we are running, due to the shelleyan JAR transformations we've done. This is where I think a different and more expansive kind of testing can help.</p> <p>So often, developers say \"Don't test third-party dependencies\", by which they mean the database, the container runtime etc. I see this slightly more subtely. If you can write fast, deterministic, easily maintainable tests that give you great confidence by incorporating your third-party dependencies, I think you should do it. However, if those tests would be flaky, slow and burdensome, then you should not do it. With modern hardware and software, I find that it is often possible to write nice tests that incorporate certain elements of your software's runtime that traditionally have been excluded from the scope of \"unit tests\" by puritans. These include the database or the container runtime. My suggestion is that you experiment and do what is most delightful for you and your team.</p> <p>This page is not yet finished.</p> <p>Created on 2023-10-30</p> <p>Updated on 2025-02-23</p>"},{"location":"writing/notes/chestertons-traffic-lights/","title":"Chesterton's Traffic Lights","text":"<p>This is about an instance of Chesterton's Fence from my life, that I finally solved today (23/02/2025).</p> <p>I like the parable of Chesterton's Fence. It requires humility and respect to look at something in the world that inconveniences you and say \"I don't understand, but I think somebody else does or did, and they might know or have known better than I do\".</p> <p>There is a road intersection just downstairs from my condo, that I have seen and crossed thousands of times in the 5 years I've lived in the place I currently live. I noticed early on during my time here that when crossing on one side of the intersection, the green light for pedestrians lasts longer than when crossing on the other side, despite no difference in the flow of traffic across the two different sides. I've often thought to myself, \"That's annoying and daft, why isn't it just the same!\"</p> <p>Today I finally came up with a coherent explanation as to why they aren't the same! On Sundays, the foot traffic for the side of the intersection with a shorter pedestrian green light  is significantly higher, and if the pedestrian green light lasted any longer, the number of people who would be able to leave one side of the crossing would be too great for the size of the island they cross over to on the other side, and they would spill out into the road, causing a hazard. The other side of the intersection doesn't have that problem because the foot traffic on that side there never gets as high.</p> <p>Now that I think I understand why the difference is there, I can in confidence propose how it could be better: The duration of the pedestrian green light should only be shortened on Sunday, rather than on every day. If you asked me before today, I might have said \"Just make the duration the same!\", and that would have been a hazardous mistake. I've always suspected that there was a sensible reason, and today I finally came up with something I think is really plausible, and I'm happy about that.</p> <p>Created on 2025-02-23</p> <p>Updated on 2025-02-23</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/","title":"Migrating this Wiki to fly.io","text":"<p>Note: My wiki now runs on GitHub pages, which is a better fit. I no longer have the power to edit my website inline, but that wasn't really an important feature. GitHub pages is just as free, and unlike running a wiki docker container on fly.io, I don't have to think about resources, like memory.</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#problem","title":"Problem","text":"<p>AWS is expensive. I have been paying 15 SGD/month to run a small EC2 instance. I had installed Docker on this EC2 instance, and was running this Wiki as a container, with appropriate exposed ports and environment variables. I was using the Wiki's integration with Letsencrypt to maintain the SSL certificate for the domain I was using for it: wiki.rrmoore.com.</p> <p>It's not breaking the bank, but it's more than necessary. If I were ever in a pinch, I would be cross with myself for not having saved these dollars earlier.</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#solution","title":"Solution","text":"<p>There are many cloud hosting providers now, all with their various niches. I have had a little experience with fly.io, and I love it. The documentation is complete, relevant and discoverable. I find the tool easy to use. Applications are configured using files, and not in a GUI. The CLI tool's output contains useful information. I'm a big fan of fly.io.</p> <p>Not so long ago, they introduced a postgres application offering. I like the way they did it - there is not very much abstraction. It is provided more as a layer on top of the ordinary container application offering. That is, it will run a postgres app as a container, and provide additional application-specific information to you. It is a great example of something I love: Design patterns that find usage in abstract ways outside of just code. Fly's postgres offering is an example of the decorator pattern applied to their product offering. The benefits are similar to those we find in code, flexibility and reuse.</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#implementation","title":"Implementation","text":"<p>Having decided that I was going to move my Wiki to fly.io using their postgres offering, I took the following steps to complete it.</p> <ol> <li>Take a logical backup of the existing Wiki's database.</li> <li>Create the new database using Fly</li> <li>Restore the backup into the new database</li> <li>Prepare the new Wiki image</li> <li>Deploy the new Wiki application</li> <li>$$$</li> </ol>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#1-take-a-logical-backup","title":"1. Take a logical backup","text":"<p>I took a logical backup (rather than a physical backup) for a couple of reasons.</p> <p>(a) The database is pretty small (b) I don't want to think about how I'm going to access the data volumes for the target database</p> <p>This later turned out to be necessary also because the target database is PG14 whereas the source database was PG11.</p> <p>I took the logical backup like this:</p> <pre><code>rob@Robs-MacBook-Pro-2 aws % ssh -i ~/.aws/wiki-ec2-keypair.pem ubuntu@54.169.244.121\nWelcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-1022-aws x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Sat Jan 14 05:25:44 UTC 2023\n\n  System load:                      0.0\n  Usage of /:                       75.6% of 7.69GB\n  Memory usage:                     50%\n  Swap usage:                       0%\n  Processes:                        130\n  Users logged in:                  0\n  IPv4 address for br-e6e84ec28b7a: 172.18.0.1\n  IPv4 address for docker0:         172.17.0.1\n  IPv4 address for eth0:            172.31.26.15\n\n * Ubuntu Pro delivers the most comprehensive open source security and\n   compliance features.\n\n   https://ubuntu.com/aws/pro\n\n88 updates can be applied immediately.\n21 of these updates are standard security updates.\nTo see these additional updates run: apt list --upgradable\n\n\n*** System restart required ***\nLast login: Sun Nov  6 13:24:20 2022 from 218.186.139.102\nubuntu@ip-172-31-26-15:~$\nubuntu@ip-172-31-26-15:~$ docker exec db pg_dump wiki -U wiki &gt; backup.sql\nubuntu@ip-172-31-26-15:~$ head backup.sql \n--\n-- PostgreSQL database dump\n--\n\n-- Dumped from database version 11.15 (Debian 11.15-1.pgdg90+1)\n-- Dumped by pg_dump version 11.15 (Debian 11.15-1.pgdg90+1)\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nubuntu@ip-172-31-26-15:~$\nubuntu@ip-172-31-26-15:~$ logout\nConnection to 54.169.244.121 closed.\nrob@Robs-MacBook-Pro-2 aws %\nrob@Robs-MacBook-Pro-2 aws % scp -i ~/.aws/wiki-ec2-keypair.pem ubuntu@54.169.244.121:~/backup.sql db/\n...\n</code></pre> <p>At this point, I had the <code>backup.sql</code> for restoring the database available to me. Next, I needed to create the new database, and restore the backup into it.</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#2-create-the-new-database-using-fly","title":"2. Create the new database using Fly","text":"<p>I created a postgres database using the <code>fly</code> CLI tool.</p> <pre><code>rob@Robs-MacBook-Pro-2 db % fly pg create --name rob-wiki-pg --region sin                \nautomatically selected personal organization: Rob Moore\n? Select configuration: Development - Single node, 1x shared CPU, 256MB RAM, 1GB disk\nCreating postgres cluster in organization personal\nCreating app...\nSetting secrets on app rob-wiki-pg...Provisioning 1 of 1 machines with image flyio/postgres:14.6\nWaiting for machine to start...\nMachine 3287d97b035d85 is created\n==&gt; Monitoring health checks\n  Waiting for 3287d97b035d85 to become healthy (started, 3/3)\n\nPostgres cluster rob-wiki-pg created\n  Username:    postgres\n  Password:    &lt;&lt;redacted&gt;&gt;\n  Hostname:    rob-wiki-pg.internal\n  Proxy port:  5432\n  Postgres port:  5433\n  Connection string: postgres://postgres:&lt;&lt;redacted&gt;&gt;@rob-wiki-pg.internal:5432\n\nSave your credentials in a secure place -- you won't be able to see them again!\n\nConnect to postgres\nAny app within the Rob Moore organization can connect to this Postgres using the following credentials:\nFor example: postgres://postgres:&lt;&lt;redacted&gt;&gt;@rob-wiki-pg.internal:5432\n\n\nNow that you've set up postgres, here's what you need to understand: https://fly.io/docs/reference/postgres-whats-next/\nrob@Robs-MacBook-Pro-2 db % \n</code></pre> <p>By default, the database is accessible only from another Fly app. In order to restore the database backup into this new database, I'll need to be able to connect to it from my machine using <code>psql</code>.</p> <p>To make the database externall accessible, there are some convenient instructions here: https://fly.io/docs/postgres/connecting/connecting-external/. Here's what I did:</p> <pre><code>rob@Robs-MacBook-Pro-2 db % fly ips allocate-v4 -a rob-wiki-pg                               \nVERSION IP              TYPE    REGION  CREATED AT \nv4      137.66.26.46    public  global  6s ago          \n\nrob@Robs-MacBook-Pro-2 db %\nrob@Robs-MacBook-Pro-2 db % fly config save -a rob-wiki-pg\nWrote config file fly.toml\nrob@Robs-MacBook-Pro-2 db %\n</code></pre> <p>When making Fly's postgres external accessible, you need to edit the <code>fly.toml</code> to add some config. Here are the changes, which are described in better detail in the documentation page I shared above.</p> <pre><code>- internal_port = 80\n+ internal_port = 5432 \n...\n+   [[services.ports]]\n+     handlers = [\"pg_tls\"]\n+    port = 5432\n</code></pre> <p>Having updated the configuration, I needed to apply it, which is done by redeploying the application. The documentation notes the importance of using the correct postgres major version for the image, which you need to re-specify when using this command.</p> <pre><code>rob@Robs-MacBook-Pro-2 wiki % fly deploy . -a rob-wiki-pg --image flyio/postgres:14\n...\nrob@Robs-MacBook-Pro-2 wiki %\n</code></pre> <p>Having done this, I was able to test my connection using my local <code>psql</code>, and it worked fine.</p> <pre><code>rob@Robs-MacBook-Pro-2 wiki % psql --version\npsql (PostgreSQL) 14.5\nrob@Robs-MacBook-Pro-2 db % psql postgres://postgres:&lt;&lt;redacted&gt;&gt;@rob-wiki-pg.fly.dev:5432     \npsql (14.5, server 14.6 (Debian 14.6-1.pgdg110+1))\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\nType \"help\" for help.\n\npostgres=#\n</code></pre> <p>With the ability to connect to the new database, I needed to set it up with a few things.</p> <p>I want a specific logical database for the Wiki application, and it should use its own user.</p> <p>This can be done using <code>psql</code>:</p> <pre><code>rob@Robs-MacBook-Pro-2 db % psql postgres://postgres:&lt;&lt;redacted&gt;&gt;@rob-wiki-pg.fly.dev:5432     \npsql (14.5, server 14.6 (Debian 14.6-1.pgdg110+1))\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\nType \"help\" for help.\n\npostgres=# CREATE DATABASE wiki;\nCREATE DATABASE \npostgres=# CREATE USER wiki;\nCREATE ROLE\npostgres=# ALTER USER wiki WITH ENCRYPTED PASSWORD '&lt;&lt;redacted&gt;&gt;';\nALTER ROLE\npostgres=# GRANT ALL PRIVILEGES ON DATABASE wiki TO wiki;\nGRANT\npostgres=# ^D\\q\nrob@Robs-MacBook-Pro-2 db %\n</code></pre>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#3-restore-the-backup-into-the-new-database","title":"3. Restore the backup into the new database","text":"<p>A logical postgres backup is a SQL script, which you can execute on your database server in order to recreate the original database.</p> <p>So you can restore a logical backup to the database of your choice, using the user of your choice, like this:</p> <pre><code>rob@Robs-MacBook-Pro-2 db % psql postgres://wiki:&lt;&lt;redacted&gt;&gt;@rob-wiki-pg.fly.dev:5432/wiki -f backup.sql \nSET\nSET\n...\nALTER TABLE\nrob@Robs-MacBook-Pro-2 db %\n</code></pre> <p>To satisfy yourself, you can check the relations inside the database. In this case, the application places its relations in the public schema.</p> <pre><code>rob@Robs-MacBook-Pro-2 db % psql postgres://wiki:&lt;&lt;redacted&gt;&gt;@rob-wiki-pg.fly.dev:5432/wiki              \npsql (14.5, server 14.6 (Debian 14.6-1.pgdg110+1))\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\nType \"help\" for help.\n\nwiki=&gt; \\dt\n             List of relations\n Schema |       Name       | Type  | Owner \n--------+------------------+-------+-------\n public | analytics        | table | wiki\n             List of relations\n Schema |       Name       | Type  | Owner \n--------+------------------+-------+-------\n public | analytics        | table | wiki\n public | apiKeys          | table | wiki\n public | assetData        | table | wiki\n public | assetFolders     | table | wiki\n public | assets           | table | wiki\n public | authentication   | table | wiki\n public | brute            | table | wiki\n public | commentProviders | table | wiki\n public | comments         | table | wiki\n public | editors          | table | wiki\n public | groups           | table | wiki\n public | locales          | table | wiki\n public | loggers          | table | wiki\n public | migrations       | table | wiki\n public | migrations_lock  | table | wiki\n public | navigation       | table | wiki\n public | pageHistory      | table | wiki\n public | pageHistoryTags  | table | wiki\n public | pageLinks        | table | wiki\n public | pageTags         | table | wiki\n public | pageTree         | table | wiki\n public | pages            | table | wiki\n public | renderers        | table | wiki\n public | searchEngines    | table | wiki\n public | sessions         | table | wiki\n public | settings         | table | wiki\n public | storage          | table | wiki\n public | tags             | table | wiki\n public | userAvatars      | table | wiki\n public | userGroups       | table | wiki\n public | userKeys         | table | wiki\n public | users            | table | wiki\n(32 rows)\n\nwiki=&gt; \nwiki=&gt; ^D\\q\nrob@Robs-MacBook-Pro-2 db % \n</code></pre>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#4-prepare-the-new-wiki-image","title":"4. Prepare the new Wiki image","text":"<p>I used this as my <code>Dockerfile</code>:</p> <pre><code>FROM ghcr.io/requarks/wiki:2\n\nENV DB_TYPE postgres\nENV DB_HOST rob-wiki-pg.fly.dev\nENV DB_PORT 5432\nENV DB_USER wiki\nENV DB_NAME wiki\n\nENV UPGRADE_COMPANION 1\n</code></pre> <p>For the database password (environment variable <code>DB_PASS</code>), we'll set that later using a secret.</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#5-deploy-the-new-wiki-application","title":"5. Deploy the new Wiki application","text":"<p>I fearlessly deployed the application. It did not work first time, because I had overlooked a couple of things.</p> <pre><code>rob@Robs-MacBook-Pro-2 app % fly launch --dockerfile Dockerfile --name rob-wiki --region sin --now\n==&gt; Verifying app config\n--&gt; Verified app config\n==&gt; Building image\n...\n097b20a4c610: Pushed \nded7a220bb05: Pushed \ndeployment-01GPQ78SG6S8Q1H669KZVNKS73: digest: sha256:211ed0fd1302d95e1b002af205b387ecf62ae3ef19581407e50aa35d846d8896 size: 3042\n--&gt; Pushing image done\nimage: registry.fly.io/rob-wiki:deployment-01GPQ78SG6S8Q1H669KZVNKS73\nimage size: 466 MB\n==&gt; Creating release\n--&gt; release v2 created\n\n--&gt; You can detach the terminal anytime without stopping the deployment\n==&gt; Monitoring deployment\nLogs: https://fly.io/apps/rob-wiki/monitoring\n\nv0 is being deployed\n...\n^C\nrob@Robs-MacBook-Pro-2 app %\n</code></pre> <p>This first deployment did not work for two reasons.</p> <ol> <li>I needed to edit the <code>fly.toml</code> to update the internal port for the container (it is 3000, rather than the default vlaue, 8080).</li> <li>I needed to set the database secret so that the application can actually connect to the new database.</li> </ol> <p>After editing the fly.toml, I added a database secret using the appropriate Fly CLI command, which then immediately redeployed the application, having seen that the configuration had changed.</p> <pre><code>rob@Robs-MacBook-Pro-2 app % flyctl secrets set DB_PASS=\"&lt;&lt;redacted&gt;&gt;\"\n\nRelease v1 created\n==&gt; Monitoring deployment\nLogs: https://fly.io/apps/rob-wiki/monitoring\n\n 1 desired, 1 placed, 1 healthy, 0 unhealthy [health checks: 1 total, 1 passing]\n--&gt; v1 deployed successfully\n\nrob@Robs-MacBook-Pro-2 app %\n</code></pre> <p>While the deployment was happening, I was monitoring the logs at the URL provided to me by the CLI tool: https://fly.io/apps/rob-wiki/monitoring. Here I was able to see the following encouraging log messages:</p> <pre><code> 2023-01-14T04:57:05.026 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.025Z [MASTER] info: No new search engines found: [ SKIPPED ]\n2023-01-14T04:57:05.215 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.215Z [MASTER] info: No new storage targets found: [ SKIPPED ]\n2023-01-14T04:57:05.216 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.216Z [MASTER] info: Checking for installed optional extensions...\n2023-01-14T04:57:05.229 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.229Z [MASTER] info: Optional extension git is installed. [ OK ]\n2023-01-14T04:57:05.238 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.238Z [MASTER] info: Optional extension pandoc was not found on this system. [ SKIPPED ]\n2023-01-14T04:57:05.244 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.244Z [MASTER] info: Optional extension puppeteer was not found on this system. [ SKIPPED ]\n2023-01-14T04:57:05.246 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.245Z [MASTER] info: Optional extension sharp was not found on this system. [ SKIPPED ]\n2023-01-14T04:57:05.252 app[1d9b7b2d] sin [info] 2023-01-14T04:57:05.251Z [MASTER] info: Authentication Strategy Local: [ OK ]\n2023-01-14T04:57:06.170 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.170Z [MASTER] info: (COMMENTS/DEFAULT) Initializing...\n2023-01-14T04:57:06.170 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.170Z [MASTER] info: (COMMENTS/DEFAULT) Initialization completed.\n2023-01-14T04:57:06.224 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.223Z [MASTER] info: Purging orphaned upload files...\n2023-01-14T04:57:06.225 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.225Z [MASTER] info: Syncing locales with Graph endpoint...\n2023-01-14T04:57:06.226 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.226Z [MASTER] info: Fetching latest updates from Graph endpoint...\n2023-01-14T04:57:06.248 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.248Z [MASTER] info: Purging orphaned upload files: [ COMPLETED ]\n2023-01-14T04:57:06.357 app[1d9b7b2d] sin [info] Loading configuration from /wiki/config.yml... OK\n2023-01-14T04:57:06.463 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.462Z [JOB] info: Rebuilding page tree...\n2023-01-14T04:57:06.546 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.545Z [MASTER] info: Fetching latest updates from Graph endpoint: [ COMPLETED ]\n2023-01-14T04:57:06.897 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.896Z [MASTER] info: Pulled latest locale updates for English from Graph endpoint: [ COMPLETED ]\n2023-01-14T04:57:06.902 app[1d9b7b2d] sin [info] 2023-01-14T04:57:06.901Z [MASTER] info: Syncing locales with Graph endpoint: [ COMPLETED ]\n2023-01-14T04:57:07.175 app[1d9b7b2d] sin [info] 2023-01-14T04:57:07.174Z [JOB] info: Using database driver pg for postgres [ OK ]\n2023-01-14T04:57:07.422 app[1d9b7b2d] sin [info] 2023-01-14T04:57:07.421Z [JOB] info: Rebuilding page tree: [ COMPLETED ] \n</code></pre>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#6","title":"6. $$$","text":"<p>My new Wiki is now available at: https://rob-wiki.fly.dev/ and is now free for me to run, versus the comparitive fortune I was paying on AWS.</p> <p>The first thing I did was write up this tutorial. I created the material as I was doing the migration. Continuous documentation in this way is a useful practice and I recommend it to everyone. My process involved a little more trial and error than is reflected in this log, but it did essentially go like this.</p>"},{"location":"writing/tutorials/migration-this-wiki-to-fly-io/#backups","title":"Backups","text":"<p>Here is my patented backup technology:</p> <pre><code>#!/bin/bash\n\nif [[ $0 != \"./backup\" ]]; then\n  echo \"Please run me from the db/ directory.\"\n  exit 1\nfi\n\nDB_PASS=$(cat db-password.txt)\nPREFIX=$(date +\"%Y-%m-%d_%H-%M-%S\")\nBACKUP_FILE=\"backups/${PREFIX}-backup-FLY.sql\"\npg_dump \"postgres://wiki:${DB_PASS}@rob-wiki-pg.fly.dev:5432/wiki\" &gt; \"$BACKUP_FILE\"\necho \"Saved $BACKUP_FILE\"\n</code></pre> <p>I execute it sometimes. I am destined for inconvenient data loss in the future.</p> <p>Created on 2023-01-13</p> <p>Updated on 2023-01-14</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/","title":"Ukkonen's Algorithm for Mortals","text":""},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#what-is-ukkonens-algorithm","title":"What is Ukkonen's algorithm?","text":"<p>Ukkonen's algorithm is an online suffix tree construction algorithm which runs in time and space that is linear with the length of the string being indexed. Below I'll break this sentence down into parts.</p> <ul> <li> <p>Ukkonen's is a suffix tree construction algorithm. This means that it accepts a string input, and as an output it produces a suffix tree for that string. There is plenty written about suffix trees, and the other explanations will be better than mine, so please look for them instead.</p> </li> <li> <p>Ukkonen's runs in time and space that is linear with the length of the input string. This means that as the length of the input increases, the corresponding increase in storage space for the resulting data structure, and the running time for its construction, increases only proportionally, and not more than that. In practice, it means that the algorithm is effective for use on much larger inputs than super-linear algorithms, which experience prohibitive costs when running for inputs above a certain length.</p> </li> <li> <p>Ukkonen's is an online algorithm. This means that each iteration of the algorithm yields a result which is correct (or can be made correct in constant time) for the input that has been processed so far.</p> </li> </ul>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#why-does-ukkonens-algorithm-exist","title":"Why does Ukkonen's algorithm exist?","text":"<p>At the time of Ukkonen's algorithm, the other known linear-time algorithms for suffix tree construction were more difficult to understand, and they lacked the desirable online property. Of the existing linear-time suffix tree construction algorithms, Ukkonen's is generally recognised to be the easiest to understand. Of course, that is highly subjective. I didn't learn the other algorithms, so I can't tell you for sure. In my opinion this one is complicated enough.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#prerequisite-knowledge","title":"Prerequisite knowledge","text":"<p>I will assume that you know what a suffix tree is. The descriptions online and in YouTube are good enough to understand what a suffix tree is, even if they are insufficient for understanding Ukkonen's construction algorithm. It would also be helpful for you to feel comfortable with the much simpler, naive, cubic time construction algorithm which is often presented as an introductory algorithm on the way to Ukkonen's.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#high-level-overview","title":"High-level overview","text":"<p>In Ukkonen's algorithm, we construct the tree iteratively, considering one character at a time from the input, scanning from the start to the end. We start with just a root node. For each new character, we extend the tree we currently have, in order to express the new suffixes in it.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#implicit-vs-canonical-suffix-trees","title":"Implicit vs canonical suffix trees","text":"<p>Each step in the algorithm produces an implicit suffix tree. An implicit suffix tree is different to a true suffix tree, aka a canonical suffix tree. An implicit suffix tree still contains all suffixes of the string, but not all of those end in a leaf node. That is, some suffixes are represented by paths in the tree from the root node, which terminate within edge labels in the tree. In order to utilise some suffix tree algorithms, we would really like to construct a canonical suffix tree, so the last step in Ukkonen's algorithm is tree canonization, which transforms the implicit suffix tree into a canonical suffix tree.</p> <p>Each implicit suffix tree produced by the algorithm in each iteration corresponds to a prefix of the input string, and every iteration's implicit suffix tree is built on top of the one from the previous iteration.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#suffix-tree-canonization","title":"Suffix tree canonization","text":"<p>To canonize an implicit suffix tree, we can add a unique character to the suffix tree at the end, after all our input has been processed and incorporated into the implicit suffix tree that we're iterating on. In almost all descriptions, a dollar symbol is used (i.e. '$'). In practice, my implementation uses a null character (i.e. '\\u0000').</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#remainder","title":"Remainder","text":"<p>Throughout the algorithm, a positive integer variable stores the number of suffixes of the so-far-consumed input string, which are not explicitly expressed in the tree. Equivalently, it represents the number of suffixes that are expressed in the tree only implicitly, and not explicitly. Every phase begins by incrementing this counter, since we know that once we add a new character to the string, there is an additional implicitly represented suffix in the tree. In most descriptions of Ukkonen's algorithm, this variable is called something like \"remainder\". In my implementation, it is called \"remainingSuffixes\". Call it whatever you're comfortable with.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#phases-and-extensions","title":"Phases and Extensions","text":"<p>The algorithm's iteration contains Phases and Extensions. For each character we are adding, we perform a Phase. For each Phase, we perform one or more Extensions. The Phase is the outer loop, and the Extension is the inner loop.</p> <p>Every Phase corresponds to a character in the input string, and equivalently, a prefix of the input string. Every Extension corresponds to a suffix of the Phase's corresponding prefix of the input string.</p> <p>Something to note is that while a Phase corresponds to a prefix of the total input string, it corresponds to the entirety of the string that is incorporated into the implicit suffix tree at the end of the Phase. Another way to think about it is that in each Phase, every suffix of the so-far consumed input string is inserted into the tree, either implicitly or explicitly. The insertion of these suffixes to the tree is done using Suffix Extensions.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#suffix-extensions","title":"Suffix Extensions","text":"<p>When we express new suffixes, we perform an action to the tree called a Suffix Extension. In a Suffix Extension, we add a new suffix of the so-far consumed input string, into the suffix tree. After the Suffix Extension, the suffix will be either implicitly or explicitly represented in the implicit suffix tree. There are a few different kinds of Suffix Extension.</p> <ul> <li>Rule 1 Extension: When the suffix we're adding can be added by extending the labels of edges leading to leaf nodes.</li> <li>Rule 2 Extension: When the suffix we're adding requires the addition of a new leaf node.</li> <li>Rule 3 Extension: When the suffix we're adding is already implicitly present in the tree, even if it is not explicitly   represented.</li> </ul> <p>Only Rule 2 Extensions result in new suffixes being made explicit in the tree. During the algorithm, while we're keeping track of the number of suffixes remaining from the portion of the input which has been processed, we need to remember to decrement this counter after each time that we perform a Rule 2 Extension.</p> <p>When we do a Rule 3 Extension, it means that the suffix we were adding to the tree is already implicitly present in the tree. This additionally means that all the suffixes of that suffix are also implicitly present in the tree, which means that none of them would result in a Rule 2 Extension. So a Rule 3 Extension ends the current Phase.</p> <p>In every Extension of every Phase, exactly one Suffix Extension happens. That is why the inner loop iterations of the algorithm are called Extensions.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#first-extension","title":"First Extension","text":"<p>The first extension of every phase is always the same. It involves extending all labels of edges to leaf nodes, so that they end in what is now the new last character in the string represented by the implicit suffix tree. The first extension of every phase is a Rule 1 Extension, and no other extensions are ever Rule 1 Extensions.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#second-extension-onwards","title":"Second Extension onwards","text":"<p>In order to run in linear time, Ukkonen's algorithm depends on being able to perform each Suffix Extension in constant time. In order to do this, every extension needs to avoid traversing the tree to find the right spot to perform any modifications to the tree. Instead, it needs to be able to find the right place in the tree to perform the Suffix Extension, in constant time. Solving this problem is key in Ukkonen's algorithm.</p> <p>The naive way to complete the Suffix Extensions in the second Extension onwards, would be to traverse the tree for each insertion, in order to find the correct place to make the Suffix Extension. This would work correctly, but traversing the tree is expensive. If we did that for all extensions, we would end up with an algorithm whose worst case running time is cubic in the length of the input string. What we're looking for is an algorithm whose worst case running time is linear in the length of the input string. Ukkonen's description of his algorithm begins with a description of this cubic-time algorithm, and presents the actual Ukkonen's algorithm as a collection of optimizations on top of it. There are two key parts of Ukkonen's optimization over the naive algorithm. They are the Active Point, and Suffix Links.</p> <p>The Active Point, combined with the use of Suffix Links, enables us to be able to complete each Extension in constant time. The Active Point locates the precise insertion point for the next Suffix Extension, and Suffix Links store exploitable information about the suffixes in the text.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#active-point","title":"Active Point","text":"<p>The Active Point is a reference to a precise location in the tree with a special property, that it will be the correct location for the next Suffix Extension to proceed from.</p> <p>This reference is represented as a triple of \"activeNode\", \"activeEdge\" and \"activeLength\". These three pieces of data combined allow us to pinpoint an exact location in the tree (and therefore an exact location in the text), for example, directly on top of an internal node, or perhaps partway into an edge label.</p> <ul> <li> <p>The \"activeNode\" variable is a pointer to either the root node, or an internal node in the tree.</p> </li> <li> <p>The \"activeEdge\" variable is a reference to a character in the text, used to indicate which edge of the tree node indicated by the \"activeNode\" we need to move along to reach the precise position of the Active Point. At any point in time, the \"activeEdge\" should be pointing to a character in the text within the suffix that is being added in the current Extension.</p> </li> <li> <p>The \"activeLength\" variable indicates how far we need to travel along the edge indicated by the \"activeEdge\", in order to get to the precise position of the Active Point.</p> </li> </ul>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#suffix-links","title":"Suffix Links","text":"<p>A Suffix Link represents a pointer going from one internal node in the tree to another internal node, such that the path to the destination node is a suffix of the path to the source node. The destination node is said to be the Suffix Link of the source node. A node can have no more than one Suffix Link. If no Suffix Link exists for a node, then it has an implicit Suffix Link going to the root node. In a given tree, many nodes will likely have a Suffix Link. Suffix Links are essential information that we store in the tree for Ukkonen's algorithm. Even when implementing other suffix tree algorithms, we may still be able to exploit the Suffix Links that were created during the tree's construction.</p> <p>A given node can only have one Suffix Link. Also, there is only ever one node with a Suffix Link to a given node. Another way I would phrase this is that a node can have no more than one inbound Suffix Link pointer, and no more than one outbound Suffix Link pointer. A node may have both an inbound Suffix Link pointer and an outbound Suffix Link pointer, that is, it may both be the Suffix Link node for another node, and also have its own Suffix Link node.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#constant-time-traversal-using-the-active-point-and-suffix-links","title":"Constant-time traversal using the Active Point and Suffix Links","text":"<p>When we perform Suffix Extensions, rather than traversing the tree from the root to find the insertion point, we instead traverse from the Active Point. In order for the algorithm to be correct, the Active Point needs to move around the tree and at the start of each Extension, it needs to somehow be exactly where it needs to be. After every Suffix Extension, two things need to happen.</p> <ul> <li> <p>The Active Point moves through the tree to ensure that it is in the right place. It may follow Suffix Links when doing this movement.</p> </li> <li> <p>Suffix Links are lain down, so that future iterations will be able to traverse the tree correctly.</p> </li> </ul> <p>In both of these actions, different things will need to be done depending on the Extension Rule that was applied for the Suffix Extension.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#active-point-movement-after-extensions","title":"Active Point movement after Extensions","text":"<p>After performing a Suffix Extension, we need to make sure that the Active Point is in precisely the right place before starting the next Suffix Extension. The hard work to figure out how to do this was done by Ukkonen in his paper, in which he provides how to move the Active Point, and the proof for why it ensures correctness. For implementors, it suffices to</p> <ul> <li> <p>On Rule 3 Extension: Whenever we perform a Rule 3 Extension, our Active Point advances one position down the tree in its current direction. This is often implemented as incrementing the \"activeLength\" variable.</p> </li> <li> <p>On Rule 2 Extension from root: Whenever we perform a Rule 2 Extension where the Active Point is along an outbound edge from the root node (but not directly on the root node), then we step the Active Point one position backwards and pull the active edge forwards to the first character of the next suffix being added. This is often implemented as decrementing the \"activeLength\" variable, and setting the \"activeEdge\" variable to point to the first character of the next suffix to be added, which you can find by stepping back from the end of the currently processed input, by a number of steps equal to the number of remaining suffixes that are not explicit in the tree.</p> </li> <li> <p>On Rule 2 Extension from an internal node: Whenever we perform a Rule 2 Extension where the Active Point is either on an internal node, or on an outbound edge from an internal node, then the Active Point follows the Suffix Link from that internal node. As mentioned earlier, if an internal node hasn't yet been assigned a Suffix Link, it has an implicit Suffix Link which is the root node, so in that case, the Active Point would go to the root node. This is often implemented by setting the \"activeNode\" variable to equal the node pointed at by the Suffix Link, or if there's none, then \"activeNode\" gets set to the root node.</p> </li> </ul>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#suffix-link-placement-after-extensions","title":"Suffix Link placement after Extensions","text":"<p>For the Active Point to be able to move around the suffix tree as it needs to, we need to lay down links between certain nodes in the tree. Specifically, if a node has a path from the root whose edge labels spell out a suffix of some other node's path from the root, then the former node is called the Suffix Link node of the latter. Often, the term Suffix Link is used to describe the destination node itself, and not just the link between the nodes.</p> <p>As mentioned earlier, every internal node has an implicit Suffix Link to the root node. Leaf nodes do not have Suffix Links.</p> <p>In each Phase, we create a Suffix Link chain, also called a Suffix Chain, which is a linked list of nodes that are connected by Suffix Links. Every time we create a new Suffix Link, the destination node of the link becomes the source node for the next Suffix Link. For this reason, rather than writing about adding a Suffix Link, I will write about adding a node to the Suffix Chain. This makes it clearer that every time a node is added, it enters the role of both a destination for a Suffix Link, and potentially a source, if there is another Suffix Link added before the end of the Phase.</p> <p>Suffix Links are created as a part of all Rule 2 and Rule 3 Suffix Extensions. This is because during any Phase, in every Extension from the second Extension onwards, the suffix of the input string being added to the tree will also be a suffix of the string added in the previous Extension (which is itself a suffix of the input string).</p> <p>The following description of the Suffix Link placement scenarios will assume that the reference to the Active Point is canonical (see next section to understand Active Point \"canonical references\") at the time when the Suffix Extension is performed on the tree.</p> <ul> <li> <p>On Rule 3 Extension: For the edge that the Active Point along, add the source node of that edge to the Suffix Chain. This will not result in repeatedly adding the same node to the suffix chain, because a Rule 3 Extension will also result in the end of the Phase, and so the Suffix Chain will be reset.</p> </li> <li> <p>On Rule 2 Extension: For the leaf node that was inserted, add its parent node to the Suffix Chain.</p> </li> </ul>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#active-point-non-canonical-references","title":"Active Point non-canonical references","text":"<p>As mentioned earlier, the Active Point reference is stored with three variables: \"activeNode\", \"activeEdge\", \"activeLength\". It is possible for the algorithm to reach a state where the \"activeLength\" indicates a length which is longer than the label for the edge indicated by the \"activeEdge\" variable. In this case, the position of the Active Point is somewhere beyond the end of that edge. A reference in this form still correctly describes a precise location for the Active Point, but we have to traverse the tree to get there. This kind of reference is called a non-canonical reference.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#active-point-reference-canonization","title":"Active Point reference canonization","text":"<p>If we find ourselves in a position where we're about to perform a Suffix Extension from a non-canonical reference, we need to stop and canonize our reference first. I don't have a rigorous argument for this, but I can confidently say that attempting to implement all the Active Point movements and Suffix Link placements from a non-canonical reference is more difficult to get right, or to follow as a reader of the code.</p> <p>Canonizing an Active Point reference is an iterative process, by which you traverse edge labels in the tree and convert your current reference into a reference which is progressively closer to the canonical one. One way to think about this, is that while you are canonizing an Active Point reference, the \"activeLength\" variable in your Active Point reference should always be decreasing, the \"activeEdge\" should point to a character closer to the end of the suffix being added (i.e. closer to the end of the input string), and the \"activeNode\" should move closer to the base of the tree.</p> <p>For me, the \"activeEdge\" was the trickiest to think about here. Something that really helped me was when I stopped thinking of it as a character, and instead treated it like an index into the text. For character comparisons at nodes, my implementation dereferences the \"activeEdge\" index of the input string to get the underlying character.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#active-point-behaviour-around-node-boundaries","title":"Active Point behaviour around node boundaries","text":"<p>If your Active Point is walking down an edge label, and reaches the next internal node, it is important to understand how the next Suffix Extension behaves. There are a few different ways to handle this in implementation, but the essential crux is making sure that all the variables in your Active Point reference are consistent with each other, and that whatever they are, the rest of the implementation is expecting the way that you handle it in the reference, including the Suffix Extension implementations, and also your Active Point reference canonization.</p> <p>For example, in my implementation, when the Active Point reaches the next node (i.e. the \"activeLength\" is at least as big as the length of the edge referenced by the \"activeEdge\"), my implementation will initiate Active Point reference canonization. This means that all of my Suffix Extension logic is expecting the Active Point reference to be canonical.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#remarks","title":"Remarks","text":"<p>This algorithm is not like many of the other, more trivial algorithms that I met in undergraduate computer science, such as back-propagation, spanning tree construction, or the simplex iteration. One of the most difficult aspects for me coming into this algorithm was that lack of high-quality information. No other tutorial-like resource besides this one, as far as I am aware, makes it clear exactly when Suffix Links need to be added.</p> <p>This title for this comes from the book \"Q for Mortals\" by Jeffry Borror.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#appendix-a-the-most-useful-sources-i-used-when-i-was-implementing-this-algorithm","title":"Appendix A: The most useful sources I used when I was implementing this algorithm","text":"<ul> <li>Ukkonen's in plain english: https://stackoverflow.com/questions/9452701/ukkonens-suffix-tree-algorithm-in-plain-english</li> <li>Tushar's tutorial on Ukkonen's: https://www.youtube.com/watch?v=aPRqocoBsFQ</li> <li>Ben Langmead's computational genomics lecture on Suffix tries and trees: https://www.youtube.com/watch?v=hLsrPsFHPcQ</li> <li>Ukkonen's original paper: https://www.cs.helsinki.fi/u/ukkonen/SuffixT1withFigs.pdf</li> <li>Brenden Kokoszka's visualisation tool: http://brenden.github.io/ukkonen-animation/</li> </ul>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#appendix-b-testing-your-implementation-for-correctness","title":"Appendix B: Testing your implementation for correctness","text":"<p>By hand, it will be difficult for you to come up with all the test cases to prove that your implementation is fully correct. I did attempt this. You can get quite far with it, but I never managed to get all the way to a robustly correct implementation by going case-by-case. I used fuzz testing in order to determine correctness.</p> <p>For fuzz testing, I recommend these guidelines:</p> <ul> <li>Use a small alphabet of characters to produce input strings</li> <li>Attempt more inputs of smaller length</li> </ul> <p>Both of these mean that the pathological cases your fuzzer finds will be simpler, which will make them easier to replay for debugging.</p>"},{"location":"writing/tutorials/ukkonens-algorithm-for-mortals/#appendix-c-implementation-with-comments","title":"Appendix C: Implementation with comments","text":"<p>This is object-oriented Kotlin.</p> <pre><code>class SuffixTree {\n    private var currentlyInsertedInput = \"\"\n    private var remainingSuffixes = 0\n\n    private val rootNode = RootNode()\n    private val activePoint = ActivePoint()\n\n    companion object {\n        fun ukkonenConstruction(input: String): SuffixTree {\n            val suffixTree = SuffixTree()\n            input.forEach { suffixTree.addChar(it) }\n            suffixTree.canonize()\n            return suffixTree\n        }\n    }\n\n    fun addChar(c: Char) {\n        // Add the character to the string of characters whose suffixes are present in the tree\n        // already.\n        currentlyInsertedInput += c\n\n        // There is now an additional suffix which is not yet explicit in the tree, so we increment\n        // our counter for the number of remaining suffixes.\n        remainingSuffixes++\n\n        // We ask the active point to add the remaining suffixes, with each suffix able to be added\n        // in O(1) time because the active point is creating and exploiting knowledge about the\n        // string's suffixes through the use of suffix links.\n        activePoint.addRemainingSuffixes(c)\n    }\n\n    /**\n     * Finds the offsets of the given query string within the input string by exploiting the built\n     * tree.\n     */\n    fun offsetsOf(queryString: String): Set&lt;Int&gt; {\n        return rootNode.offsetsOf(queryString)\n    }\n\n    /**\n     * This converts the implicit suffix tree into a canonical suffix tree by adding a character\n     * that doesn't appear elsewhere in the input.\n     */\n    private fun canonize() {\n        addChar('\\u0000')\n    }\n\n    override fun toString(): String {\n        return \"SuffixTree(rootNode={\\n$rootNode\\n})\"\n    }\n\n    inner class ActivePoint {\n        private var activeNode: Node = rootNode\n        private var activeLength = 0\n        private var activeEdge = 0\n\n        private var suffixLinkCandidate: Node? = null\n\n        /**\n         * Add the suffixes that still need to be made explicit in the tree.\n         *\n         * @param c The character we're adding to the tree in the current phase.\n         */\n        fun addRemainingSuffixes(c: Char) {\n            // We only add suffix links within a phase, so we reset it at the start of the phase.\n            suffixLinkCandidate = null\n\n            while (remainingSuffixes &gt; 0) {\n                val extensionRuleThatWasApplied = addSuffix(c)\n                if (extensionRuleThatWasApplied == SuffixExtensionRule.RULE_THREE) {\n                    // When we apply rule three extensions, it ends the current phase because the\n                    // suffix we want to add is already implicitly present in the tree.\n                    break\n                }\n\n                // Since we haven't broken out of the loop, it means we didn't do a rule three extension,\n                // which means that a new suffix has been made explicit within the tree. When this\n                // happens, we decrement the number of suffixes that still need to be added.\n                remainingSuffixes--\n\n                if (activeNode == rootNode &amp;&amp; activeLength &gt; 0) {\n                    // When we insert a node from root, we decrement our active length, and pull our\n                    // active edge forwards to point at the start of the next suffix we're adding.\n                    activeLength--\n                    activeEdge = currentlyInsertedInput.length - remainingSuffixes\n                } else {\n                    // When we insert a node from an internal node, we follow its suffix link if it has\n                    // one. The default suffix link for any node is root.\n                    activeNode = activeNode.suffixLink()\n                }\n            }\n        }\n\n        /**\n         * @param c The character we're adding to the tree in the current phase.\n         * @return Which suffix extension rule was applied in order to add the next suffix.\n         */\n        private fun addSuffix(c: Char): SuffixExtensionRule {\n            if (activeLength == 0) {\n                // If we're at a node, set our active edge at the last added character in the text.\n                activeEdge = currentlyInsertedInput.length - 1\n            }\n\n            val activeEdgeLeadingChar = currentlyInsertedInput[activeEdge]\n            val nextNode = activeNode.edges[activeEdgeLeadingChar]\n            if (nextNode == null) {\n                // If the active node doesn't yet have a child node corresponding to the next\n                // character, add one. When we perform a leaf insertion like this, we need to add a\n                // suffix link.\n                activeNode.edges[activeEdgeLeadingChar] = LeafNode()\n                addSuffixLink(activeNode)\n                return SuffixExtensionRule.RULE_TWO\n            } else {\n                // If the reference to the active point is non-canonical, then canonize it by\n                // recursively stepping through the tree, and then go to the next extension of the\n                // current phase so that we can do all our steps from the basis of a canonical\n                // reference to the active point.\n                val edgeLength = nextNode.edgeLength()\n                if (activeLength &gt;= edgeLength) {\n                    activeEdge += edgeLength\n                    activeLength -= edgeLength\n                    activeNode = nextNode\n                    return addSuffix(c)\n                }\n\n                // If the character is already present on the edge we are creating for the next\n                // node, then we perform a rule three extension. We add a suffix link to the active\n                // node, so the active point gets to the right place after our next node insertion.\n                if (currentlyInsertedInput[nextNode.start + activeLength] == c) {\n                    activeLength++\n                    addSuffixLink(activeNode)\n                    return SuffixExtensionRule.RULE_THREE\n                }\n\n                // We split the active edge by creating a new internal node and a new leaf node for\n                // the next character. We add a suffix link for the newly created internal node.\n                val internalNode = Node(nextNode.start, nextNode.start + activeLength)\n                activeNode.edges[activeEdgeLeadingChar] = internalNode\n                internalNode.edges[c] = LeafNode()\n                nextNode.start += activeLength\n                internalNode.edges[currentlyInsertedInput[nextNode.start]] = nextNode\n                addSuffixLink(internalNode)\n                return SuffixExtensionRule.RULE_TWO\n            }\n        }\n\n        private fun addSuffixLink(node: Node) {\n            suffixLinkCandidate?.linkTo(node)\n            suffixLinkCandidate = node\n        }\n    }\n\n    open inner class Node(var start: Int, private var end: Int) {\n        private var suffixLink: Node? = null\n\n        val suffix = currentlyInsertedInput.length - remainingSuffixes\n        var edges = mutableMapOf&lt;Char, Node&gt;()\n\n        fun edgeLength(): Int = minOf(end, currentlyInsertedInput.length) - start\n\n        fun suffixLink(): Node = suffixLink ?: rootNode\n\n        fun linkTo(node: Node) {\n            suffixLink = node\n        }\n\n        fun edgeLabel(): String =\n            currentlyInsertedInput.substring(start, minOf(end, currentlyInsertedInput.length))\n\n        override fun toString(): String {\n            return toString(1)\n        }\n\n        open fun toString(indentationLevel: Int): String {\n            return \"Node(start=$start, end=$end, suffix=$suffix, hasLink?=${suffixLink != null}, label=${edgeLabel()}, \" +\n                    \"edges:${\n                        edges.map {\n                            \"\\n${\"\\t\".repeat(indentationLevel)}'${it.key}'=${\n                                it.value.toString(\n                                    indentationLevel + 1\n                                )\n                            }\"\n                        }\n                    })\"\n        }\n    }\n\n    inner class LeafNode : Node(currentlyInsertedInput.length - 1, Int.MAX_VALUE) {\n        override fun toString(): String {\n            return \"LeafNode(start=$start, end=end, suffix=$suffix, label=${edgeLabel()})\"\n        }\n\n        override fun toString(indentationLevel: Int): String {\n            return toString()\n        }\n    }\n\n    inner class RootNode : Node(-1, -1) {\n        fun offsetsOf(queryString: String): Set&lt;Int&gt; {\n            var i = 0\n            var node: Node = this\n            while (i &lt; queryString.length) {\n                // We try to follow the edge to the next internal node. If there is no such edge, then\n                // there are no matches and we return the empty set.\n                node = (node.edges[queryString[i]] ?: return setOf())\n\n                // If the edge we just followed is longer than the remainder of the query string, then\n                // we get matches only if the edge label starts with the remainder of the query string.\n                // Otherwise we get no matches\n                if (node.edgeLength() &gt;= (queryString.length - i)) {\n                    var j = 0\n                    while (j &lt; queryString.length - i) {\n                        if (queryString[i + j] != currentlyInsertedInput[node.start + j]) {\n                            return setOf()\n                        }\n                        j++\n                    }\n                    return suffixesUnderSubtreeRootedAt(node)\n                }\n\n                // If the edge we just followed doesn't have an edge label matching the query string,\n                // from the current character until the end of the edge label, then there are no\n                // matches.\n                var k = 0\n                while (k &lt; node.edgeLength()) {\n                    if (queryString[i + k] != currentlyInsertedInput[node.start + k]) {\n                        return setOf()\n                    }\n                    k++\n                }\n\n                // We increase i by the size of the matching edge label we just crossed.\n                i += node.edgeLength()\n            }\n\n            // If we make it out of the loop, then we have consumed the full query string by traversing\n            // edges from the root. This means that all the suffixes stored within the current subtree\n            // will be prefixed by the query string.\n            return suffixesUnderSubtreeRootedAt(node)\n        }\n\n        private fun suffixesUnderSubtreeRootedAt(node: Node): Set&lt;Int&gt; {\n            return if (node.edges.isEmpty()) {\n                setOf(node.suffix)\n            } else {\n                node.edges.flatMap { suffixesUnderSubtreeRootedAt(it.value) }.toSet()\n            }\n        }\n\n        override fun toString(): String {\n            return \"RootNode(edges:${edges.map { \"\\n\\t'${it.key}'=${it.value.toString(2)}\" }})\"\n        }\n    }\n}\n\nprivate enum class SuffixExtensionRule {\n    /**\n     * Rule one extensions happen in the first extension of every phase, when the leaf nodes are\n     * implicitly extended by the addition of the next character. I have included it here for my\n     * documentation of the algorithm.\n     */\n    @Suppress(\"unused\")\n    RULE_ONE,\n\n    /**\n     * Rule two extensions happen when the suffix is not implicitly present in the tree, so we add\n     * it in a new leaf node.\n     */\n    RULE_TWO,\n\n    /**\n     * Rule three extensions happen when the suffix is already implicitly present in the tree, in\n     * which case we do nothing. These suffixes will be made explicit later when the tree is\n     * canonized by the addition of a unique character at the end.\n     */\n    RULE_THREE\n}\n</code></pre> <p>Created on 2022-05-28</p> <p>Updated on 2023-03-09</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/","title":"Unit Testing for Jenkins Pipeline Libraries","text":""},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#motivation","title":"Motivation","text":"<p>Almost everyone now uses automated CI pipelines. Originally they just ran the build, but in contemporary development ecosystems, pipelines have a diverse range of responsibilities, such as:</p> <ul> <li>Producing derivative build artifacts, like Docker images</li> <li>Publishing build artifacts to an artifact repository</li> <li>Deploying components into various environments</li> <li>Integrating other verification tools, like static code analysis, performance   testing and mutation testing</li> <li>Audit compliance</li> </ul> <p>The behaviour of these complex custom pipelines is determined by some kind of pipeline configuration. In this article, I'll use a Jenkins pipeline, with the pipeline definition written using Groovy. There are similar \"pipeline-as-code\" solutions for other CI servers, such as Gomatic for GoCD. TeamCity allows you to define your pipelines using its Kotlin DSL.</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#the-challenge-of-writing-automated-tests-for-pipeline-libraries","title":"The challenge of writing automated tests for pipeline libraries","text":"<p>One of the challenges with automated testing for pipeline libraries is that many of the behaviours that you expect from a pipeline don't lend themselves so obviously to unit testing. For example - publishing an artifact in a remote artifact repository.</p> <p>Additionally, your code may depend heavily on behaviour that is only available when run on a real CI agent. In Jenkins, an obvious example might be invoking shell commands. Invoking shell commands within a replicated environment for a unit test would require some fairly heavyweight setup, although it could be technically feasible.</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#the-costs-of-untested-code-still-apply-to-pipeline-libraries","title":"The costs of untested code still apply to pipeline libraries","text":"<p>Based on the above challenges faced by pipeline library maintainers, almost all the pipeline library code I've seen exists without having any kind of automated testing. The code is only exercised during manual testing, done on an actual CI agent. The different branches are tested individually, and in general, the cost of change is quite high. If you choose not to accept a high cost of change, then consumers will pay. Bugs can be missed, and pipelines may break even due to minor issues like typos. Code review will not save you from these things.</p> <p>For changes that have the potential to affect many behaviours of the pipeline library, the manual testing phase will have to cover everything. This will provide a huge barrier to broadly scoped refactorings which would otherwise have the potential to greatly improve your experience of making changes.</p> <p>A lack of tests will also discourage other developers from making changes to the pipeline library. This will make existing maintainers into bottlenecks, which is an unfortunate position to be in. This compounds with the cost of change, because not only are maintainers in a position where they are in-demand bottlenecks, but also they can't safely make refactorings which could make it possible for non-maintainers to make changes.</p> <p>This problem of bottleneck maintainers is especially bad for teams where the pipeline library maintainers are \"part-time\", that is, they are actually meant to be doing feature delivery, but they are also the team's de-facto \"CI person\". A lack of tests will cement this person's role as full-time pipeline library maintainer by making the code inaccessible to other people, who cannot safely make changes.</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#put-a-build-tool-around-your-pipeline-library-code","title":"Put a build tool around your pipeline library code","text":"<p>My friend Adrian B\u00e4rtschi introduced me to this while we worked together. We used Gradle to define a project layout that enabled us to develop the pipeline library like a normal JVM codebase, including remote dependencies and test sources.</p> <p>We organise our pipeline library code like this:</p> <ul> <li>build.gradle</li> <li>settings.gradle</li> <li>src<ul> <li>some_package<ul> <li>MyPipeline.groovy</li> </ul> </li> </ul> </li> <li>test<ul> <li>some_package<ul> <li>MyJunitTest.java</li> </ul> </li> </ul> </li> <li>vars<ul> <li>entryPoint.groovy</li> </ul> </li> </ul> <p>Our <code>build.gradle</code> looks like this:</p> <pre><code>plugins {\n  id \"groovy\"\n}\n\nrepositories {\n  mavenCentral()\n}\n\ndependencies {\n  implementation \"org.codehaus.groovy:groovy-all:2.4.15\"\n\n  testImplementation \"org.junit.jupiter:junit-jupiter:5.4.2\"\n  testImplementation \"org.mockito:mockito-core:3.5.13\"\n  testImplementation \"org.hamcrest:hamcrest:2.2\"\n  testRuntimeOnly \"org.junit.jupiter:junit-jupiter-engine\"\n}\n\ntest {\n  useJUnitPlatform()\n}\n\nsourceSets {\n  main {\n    groovy {\n      //noinspection GroovyAssignabilityCheck\n      srcDirs = [\"vars\", \"src\"]\n    }\n  }\n\n  test {\n    java {\n      //noinspection GroovyAssignabilityCheck\n      srcDirs = [\"test\"]\n    }\n  }\n}\n</code></pre> <p>With this setup, you can write you pipeline library as-normal.</p> <p>Something to note is that we're using Java for the test sources, but Groovy for the main sources. This is because we wanted to Junit for the tests, which is a tool we're really familiar with. In general, Java is also a language we are more familiar with, so we thought we'd rather use that. In practice, it probably doesn't matter much what you go with. Gradle provides an extensible build language, such as the <code>sourceSets</code> construction, which enables you to pick-and-choose based on whatever works best for your team and your context.</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#wrap-3rd-party-interfaces-using-an-adaptor-class","title":"Wrap 3rd party interfaces using an Adaptor class","text":"<p>For the interfaces that can only be realistically implemented on a CI agent, define the interface clearly and position it in your code such that you can inject test doubles for that behaviour.</p> <p>Below is an example of a Jenkins pipeline library class which wraps the CI agent runtime environment in order to provide a testable interface.</p> <p>The <code>Object jenkins</code> which is injected as a dependency in the constructor is the object which access to all the default methods available within the CI agent runtime environment. In Jenkins, this includes any methods or fields provided by Jenkins plugins and by Jenkins itself.</p> <pre><code>// Jenkins.groovy\n\n/**\n * An example of a simple wrapper object which delegates to the object \n * representing the CI agent runtime environment. This object can be mocked in\n * tests so that all the code in the pipeline library can be exercised locally\n * in unit tests.\n */\nclass Jenkins {\n    private final Object jenkins\n\n    Jenkins(Object jenkins) {\n        this.jenkins = jenkins\n    }\n\n    void node(Closure closure) {\n        jenkins.node(closure)\n    }\n\n    void stage(String name, Closure closure) {\n        println(\"--- Stage: ${name} ---\")\n        jenkins.stage(name, closure)\n    }\n\n    void println(String text) {\n        jenkins.echo(text)\n    }\n\n    void sh(String script) {\n        jenkins.sh(script)\n    }\n}\n</code></pre>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#only-write-tests-that-give-you-confidence","title":"Only write tests that give you confidence","text":"<p>In a context where almost all the behaviours you want to test are dependent on an external interface, it can be tempting to write tests which use mocks to bake in the current implementation, rather than write tests which verify behaviour that really gives you confidence.</p> <p>An example of a really simple test that gives me a very basic level of confidence is a test that just exercises all the code and makes sure that there are no typos or dynamic typing mismatches.</p> <p>A more complicated test might stub Jenkins' <code>findFiles</code> to provide a list of files in the same format as is returned by Jenkins agents' runtime, and then assert that some kind of transformations and filters were happened correctly.</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#invest-in-really-understanding-the-ci-agent-runtime","title":"Invest in really understanding the CI agent runtime","text":"<p>One of my favourite things about TDD is that you can't do it well without understanding the problem domain. It will be impossible to write many useful tests for your pipeline library if you don't understand the behaviours that you depend on, because you need to be able to stub the behaviour correctly where appropriate, and you need to know which interactions to write mock object verifications for as assertions.</p> <p>For this reason, I recommend reading up on the behaviour of the out-of-the-box Jenkins behaviours, and the behaviour of the add-on plugins which your pipeline library depends on. Understanding them well will help you to write a pipeline which works nicely, and where the tests you have for it give you confidence in your changes.</p>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#example-of-a-test","title":"Example of a test","text":"<p>Here's an example of a test from a pipeline library I wrote not too long ago as a personal project, while I was doing more exploration practically in the area of unit testing for pipeline libraries.</p> <pre><code>package stage.clone;\n\nimport org.junit.jupiter.api.Test;\nimport org.mockito.InOrder;\nimport org.mockito.Mockito;\nimport pipeline.JobParameters;\nimport pipeline.MockJenkins;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\nimport static org.mockito.ArgumentMatchers.*;\nimport static org.mockito.Mockito.verify;\n\npublic class CloneStageTest {\n\n    private final JobParameters jobParameters = new JobParameters(\"https://github.com/robmoore-i/AccountingCalisthenics\");\n\n    @Test\n    void returnsTheCodeDirectoryRelativePath() {\n        MockJenkins jenkins = new MockJenkins();\n        CloneStage stage = new CloneStage(jenkins);\n\n        assertEquals(\"code\", stage.run(jobParameters));\n    }\n\n    @Test\n    void ensuresTheCodeDirectoryIsDeletedBeforeCloning() {\n        MockJenkins jenkins = new MockJenkins();\n        CloneStage stage = new CloneStage(jenkins);\n\n        stage.run(jobParameters);\n\n        InOrder inOrder = Mockito.inOrder(jenkins.mock);\n        inOrder.verify(jenkins.mock).sh(\"rm -rf code\");\n        inOrder.verify(jenkins.mock).sh(startsWith(\"git clone\"));\n    }\n\n    @Test\n    void clonesTheGivenRepository() {\n        MockJenkins jenkins = new MockJenkins();\n        CloneStage stage = new CloneStage(jenkins);\n\n        stage.run(jobParameters);\n\n        verify(jenkins.mock).sh(\"git clone \" + jobParameters.githubUrl + \" code\");\n    }\n\n    @Test\n    void startsTheStage() {\n        MockJenkins jenkins = new MockJenkins();\n        CloneStage stage = new CloneStage(jenkins);\n\n        stage.run(jobParameters);\n\n        verify(jenkins.mock).stage(eq(\"Clone\"), any());\n    }\n}\n</code></pre>"},{"location":"writing/tutorials/unit-testing-for-jenkins-pipeline-libraries/#summary","title":"Summary","text":"<ul> <li> <p>Writing automated tests for pipeline libraries can be challenging</p> </li> <li> <p>Growing your pipeline library code without tests will result in the same   problems as you would see from any other codebase that is growing without   tests.</p> </li> <li> <p>There are a few techniques you can apply which will make writing tests for   your pipeline library easier, such as:</p> <ul> <li>Using a build tool, like Gradle</li> <li>Wrap 3rd party interfaces using an Adaptor class</li> <li>Only write tests that give you confidence</li> <li>Invest in really understanding the CI runtime</li> </ul> </li> </ul> <p>Created on 2022-05-28</p> <p>Updated on 2023-03-09</p>"}]}